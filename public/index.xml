<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> </title>
    <link>http://localhost:1313/</link>
    <description>Recent content on  </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Sat, 29 Jul 2023 19:41:07 +1000</lastBuildDate><atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Naive Bayes authorship classification with Facebook Messenger data</title>
      <link>http://localhost:1313/posts/2023/07/naive-bayes-authorship-classification-with-facebook-messenger-data/</link>
      <pubDate>Sat, 29 Jul 2023 19:41:07 +1000</pubDate>
      
      <guid>http://localhost:1313/posts/2023/07/naive-bayes-authorship-classification-with-facebook-messenger-data/</guid>
      <description>1.1 Introduction Many of us have easy access to an incredibly rich, personalised textual dataset.
I have had a Facebook profile since my early high school years. I have access to hundreds of Facebook Messenger conversations to which I have contributed for over a decade. In other words, I have access to an enormous amount of textual data, comprising not only all the written words that I’ve sent over Messenger but also the words that my friends have sent to me.</description>
    </item>
    
    <item>
      <title>Reddit and language data part 2 - Generating text</title>
      <link>http://localhost:1313/posts/2022/10/reddit-and-language-data-part-2-generating-text/</link>
      <pubDate>Mon, 03 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/2022/10/reddit-and-language-data-part-2-generating-text/</guid>
      <description>1. Introduction In this post, I’ll show you how I wrote a Python script to generate text using the bigram language model.
For my second project on this site, I wanted to learn to go beyond simply analysing language data on Reddit – I wanted to learn how to generate it.
I thought that a good first foray into language generation would be to take user comment data from a Reddit community (‘subreddit’), create a language model from the linguistic patterns of that data, and then generate new text from those patterns.</description>
    </item>
    
    <item>
      <title>Reddit and language data part 1 - Analysing text</title>
      <link>http://localhost:1313/posts/2022/01/reddit-and-language-data-part-1-analysing-text/</link>
      <pubDate>Fri, 07 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/2022/01/reddit-and-language-data-part-1-analysing-text/</guid>
      <description>Introduction I’m barely on Reddit these days.
However, when I was on Reddit, I found it interesting how different Reddit communities (‘subreddits’) seemed to have distinct flavours in the way users socially contribute to their forum.
Since Reddit is a host for a diverse group of communities banded around interests, it would follow that the type of language used in each subreddit would have its own special signature of popular word choices, sentiments, and social dynamics.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>http://localhost:1313/about/</link>
      <pubDate>Tue, 04 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/about/</guid>
      <description>Hi! My name&amp;rsquo;s Gina Welsh.
I&amp;rsquo;m using this site to document my programming projects. I am most interested in data analysis and linguistics, so you&amp;rsquo;ll probably find projects related to that here.
I&amp;rsquo;m hoping that I&amp;rsquo;ll update this site at least semi-regularly.</description>
    </item>
    
  </channel>
</rss>
