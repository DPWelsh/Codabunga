<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="map[name:Gina Welsh]">
<meta name="description" content="Introduction I’m barely on Reddit these days.
However, when I was on Reddit, I found it interesting how different Reddit communities (‘subreddits’) seemed to have distinct flavours in the way users socially contribute to their forum.
Since Reddit is a host for a diverse group of communities banded around interests, it would follow that the type of language used in each subreddit would have its own special signature of popular word choices, sentiments, and social dynamics." />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#252627" />
<link rel="canonical" href="https://codabunga.io/posts/2022/01/reddit-and-language-data-part-1-analysing-text/" />


    <title>
        
            Reddit and language data part 1 - Analysing text :: Codabunga  — Hello Friend NG Theme
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="https://codabunga.io/main.ee04b1ef6d7cb915a458244e852501a1e2945f97aa490dde64489f00f7a5943b.css">




    <link rel="apple-touch-icon" sizes="180x180" href="https://codabunga.io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://codabunga.io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://codabunga.io/favicon-16x16.png">
    <link rel="manifest" href="https://codabunga.io/site.webmanifest">
    <link rel="mask-icon" href="https://codabunga.io/safari-pinned-tab.svg" color="#252627">
    <link rel="shortcut icon" href="https://codabunga.io/favicon.ico">
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">



<meta itemprop="name" content="Reddit and language data part 1 - Analysing text">
<meta itemprop="description" content="Introduction I’m barely on Reddit these days.
However, when I was on Reddit, I found it interesting how different Reddit communities (‘subreddits’) seemed to have distinct flavours in the way users socially contribute to their forum.
Since Reddit is a host for a diverse group of communities banded around interests, it would follow that the type of language used in each subreddit would have its own special signature of popular word choices, sentiments, and social dynamics.">
<meta itemprop="datePublished" content="2022-01-07T00:00:00+00:00" />
<meta itemprop="dateModified" content="2022-01-07T00:00:00+00:00" />
<meta itemprop="wordCount" content="2724">
<meta itemprop="image" content="https://codabunga.io"/>



<meta itemprop="keywords" content="" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://codabunga.io"/>

<meta name="twitter:title" content="Reddit and language data part 1 - Analysing text"/>
<meta name="twitter:description" content="Introduction I’m barely on Reddit these days.
However, when I was on Reddit, I found it interesting how different Reddit communities (‘subreddits’) seemed to have distinct flavours in the way users socially contribute to their forum.
Since Reddit is a host for a diverse group of communities banded around interests, it would follow that the type of language used in each subreddit would have its own special signature of popular word choices, sentiments, and social dynamics."/>



    <meta property="og:title" content="Reddit and language data part 1 - Analysing text" />
<meta property="og:description" content="Introduction I’m barely on Reddit these days.
However, when I was on Reddit, I found it interesting how different Reddit communities (‘subreddits’) seemed to have distinct flavours in the way users socially contribute to their forum.
Since Reddit is a host for a diverse group of communities banded around interests, it would follow that the type of language used in each subreddit would have its own special signature of popular word choices, sentiments, and social dynamics." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://codabunga.io/posts/2022/01/reddit-and-language-data-part-1-analysing-text/" />
<meta property="og:image" content="https://codabunga.io"/>
<meta property="article:published_time" content="2022-01-07T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-01-07T00:00:00+00:00" /><meta property="og:see_also" content="https://codabunga.io/posts/2023/07/naive-bayes-authorship-classification-with-facebook-messenger-data/" /><meta property="og:see_also" content="https://codabunga.io/posts/2022/10/reddit-and-language-data-part-2-generating-text/" />







    <meta property="article:published_time" content="2022-01-07 00:00:00 &#43;0000 UTC" />








    </head>

    <body class="">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="https://codabunga.io/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">$ cd /home/</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://codabunga.io/about/">About</a></li><li><a href="https://codabunga.io/posts/">Posts</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
  <main class="post">

    <div class="post-info">
      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock">
          <circle cx="12" cy="12" r="10"></circle>
          <polyline points="12 6 12 12 16 14"></polyline>
        </svg>
        13 minutes

        
      </p>
    </div>

    <article>
      <h1 class="post-title">
        <a href="https://codabunga.io/posts/2022/01/reddit-and-language-data-part-1-analysing-text/">Reddit and language data part 1 - Analysing text</a>
      </h1>

      

      <div class="post-content">
        <h2 id="introduction">Introduction</h2>
<p>I’m barely on Reddit these days.</p>
<p>However, when I was on Reddit, I found it interesting how different Reddit communities (‘subreddits’) seemed to have distinct flavours in the way users socially contribute to their forum.</p>
<p><img src="https://codabunga.io/Reddit_homepage.png" alt="Reddit Homepage"></p>
<p>Since Reddit is a host for a diverse group of communities banded around interests, it would follow that the type of language used in each subreddit would have its own special signature of popular word choices, sentiments, and social dynamics.</p>
<p>I knew there would be fascinating ways to analyse and compare the language data of different subreddits. So, I decided to start a project where I’d scrape data from different subreddits and do some linguistic analysis on the comments. I was most interested in comparing the differences between selected subreddits.</p>
<p>For this post, I will make comparisons of word data between two subreddits:</p>
<ol>
<li>r/productivity, a subreddit that “shares tips and tricks for being more productive”, and</li>
<li>r/antiwork, a subreddit where users are “curious about ending work” and “want to get the most out of a work-free life”</li>
</ol>
<p>I chose these two subreddits for this post since they both relate to the topic of work, but they approach the topic from very different perspectives. They are also both significant communities on Reddit, with r/productivity having almost 900,000 members at the time of writing and r/antiwork having 1.5 million “idlers”. So, I made a bet that contrasting the language used in these two communities could lead to some interesting insights.</p>
<p>To start this project, I kept it very simple. My first task focused on word frequency analysis, that is, finding insights from how often certain words appear in a sample of data (e.g. the word “cup” appears 5 times in a sample paragraph).</p>
<p>In this post, I will explore how I obtained the top 20 nouns and adjectives in r/productivity and r/antiwork. Towards the end, I do a little bit of armchair analysis into why these top nouns and adjectives might occur in the sampled data.</p>
<h2 id="step-1-setting-up-the-required-python-libraries">Step 1: Setting up the required Python libraries</h2>
<p>For this step, I will presume that you’ve already got a version of Python 3 and whatever IDE you use on your computer. You’ll also need to install <!-- raw HTML omitted -->nltk<!-- raw HTML omitted --> and <!-- raw HTML omitted -->pandas<!-- raw HTML omitted --> Python libraries as well as the <!-- raw HTML omitted -->praw<!-- raw HTML omitted --> API, if you haven’t already - you can find links to each respectively <a href="https://www.nltk.org/">here</a>, <a href="https://pandas.pydata.org/docs/">here</a> and <a href="https://praw.readthedocs.io/en/stable/index.html">here</a>.</p>
<p>At the top of my script, I imported the <!-- raw HTML omitted -->nltk<!-- raw HTML omitted -->, <!-- raw HTML omitted -->praw<!-- raw HTML omitted --> and <!-- raw HTML omitted -->pandas<!-- raw HTML omitted --> libraries since I’d need these later in the script. For the <!-- raw HTML omitted -->nltk<!-- raw HTML omitted --> library, I made sure to specifically import the word_tokenize and pos_tag modules, since we’ll be needing this to analyse the words in the language data.</p>
<pre><code>import nltk
from nltk import word_tokenize
import nltk.data
from nltk.probability import FreqDist
from nltk.tag import pos_tag
import praw
import pandas as pd
</code></pre>
<h2 id="step-2-accessing-the-subreddits">Step 2: Accessing the subreddits</h2>
<p>The PRAW API was the element that connected my Python script to the actual Reddit data itself. This is the part that ‘scrapes’ the real data. For this step, I had to first <a href="https://www.reddit.com/prefs/apps/">register an application on Reddit</a>. Once this was set up, I used the following code to authenticate my script to access Reddit (the ‘XXX’ details in this code will depend on the user details of your reddit application):</p>
<pre><code># access reddit
reddit = praw.Reddit(client_id='XXX', 
                 client_secret='XXX', \
                 user_agent='XXX')
</code></pre>
<p>Next, I had to assign variables that accessed specific subreddits of r/antiwork and r/productivity. I used the reddit.subreddit() function, putting the subreddits (‘antiwork’ and ‘productivity’) into each parameter:</p>
<pre><code># assign initalising variables to the four subreddits - connect to them via reddit API
antiwork = reddit.subreddit('antiwork')
productivity = reddit.subreddit('productivity')
</code></pre>
<p>This connected my script to the r/antiwork and r/productivity subreddits.</p>
<p>Now time for the real fun!</p>
<h2 id="step-3-collecting-the-data">Step 3: Collecting the data</h2>
<p>Now that I had access to the subreddits, a question popped into my head. What type of data did I want to scrape from these communities?</p>
<p>Submissions to Reddit come in various data formats such as text, images, links, and videos. When I looked at the subreddits with my naked (human) eye, I noticed that a lot of submissions (the contributions created by ‘original poster’ users) were in the image data format. This kind of format was useless for what I was trying to do with my project since I needed actual text to make sense of the data linguistically. As a result, I narrowed the format of my data collection to subreddit comments, since they contained the most raw text that I needed for linguistic analysis.</p>
<p>I assigned variables to the <!-- raw HTML omitted -->PRAW comments()<!-- raw HTML omitted --> function that scraped 300 comments from r/antiwork and r/productivity respectively:</p>
<pre><code># sample 300 comments from each subreddit
antiwork = antiwork.comments(limit=300)
productivity = productivity.comments(limit=300)
</code></pre>
<p>This action doesn’t access the actual text from the comments, only the data objects. So, to get the actual text from the comments, I wrote a general function that can take raw text (comment.body) from each comment for any selected subreddit:</p>
<pre><code># return raw text from comments
def return_comments(community):
  return [comment.body for comment in community if discord_string not in comment.body]
</code></pre>
<p>The result of this was a list of strings that contained the raw text of 300 comments from the selected subreddit.</p>
<p>Then, I applied this function to the 300 sample comments from the two communities:</p>
<pre><code># create corpora training data for each subreddit based on top-level comments
antiwork_corpus = return_comments(antiwork)
productivity_corpus = return_comments(productivity)
</code></pre>
<p>When I printed one of these variables, the result was a list of 300 raw comment strings scraped from the selected subreddit community (the result below is an example from r/antiwork):</p>
<p><img src="https://codabunga.io/comment_strings.png" alt="Comment strings"></p>
<p>At this point, the raw textual comment data has been pulled from the selected subreddits and put into a list. Each item in this list is a long multi-word string. However, to make sense of data linguistically, we need more than just raw strings of data. We need to tokenize the data, or in other words, split the raw strings up into smaller parts such as words or punctuation. In this way, we can grapple with the linguistically meaningful parts of our data and do some analysis!</p>
<h2 id="step-4-words---tokenizing-and-analysing-parts-of-speech">Step 4: Words - tokenizing and analysing parts-of-speech</h2>
<p>Since I was looking for the top 20 nouns and adjectives in the subreddit communities, it made sense to aim for actual words in my tokenize data (as opposed to sentences or punctuation).</p>
<p>I started off writing a function that used the <!-- raw HTML omitted -->tokenize()<!-- raw HTML omitted --> module from the <!-- raw HTML omitted -->nltk<!-- raw HTML omitted --> library. Since I was also looking at all the words in the corpus (not of any specific user comments), I used the <!-- raw HTML omitted -->“”.join<!-- raw HTML omitted --> Python command to link all the comments of the corpus words together in one big list (as opposed to a list of lists).</p>
<p>I applied this function to the antiwork_corpus and productivity_corpus variables:</p>
<pre><code># tokenize corpora

def tokenize(community):
  return word_tokenize(&quot;&quot;.join(community))

tokenized_antiwork = tokenize(antiwork_corpus)
tokenized_productivity = tokenize(productivity_corpus)
</code></pre>
<p>I applied the <!-- raw HTML omitted -->nltk.pos_tag()<!-- raw HTML omitted --> function to the tokenized data for both subreddits. This would apply a part-of-speech tag (in other words, their linguistic category) to each word in the tokenized list.</p>
<pre><code># tag corpus words for part of speech
tagged_antiwork = pos_tag(tokenized_antiwork)
tagged_productivity = pos_tag(tokenized_productivity)
</code></pre>
<p>The result of this function was a list of tuples where each tuple had the word and their corresponding part-of-speech tag. You can see some examples in the printed list below:</p>
<p><img src="https://codabunga.io/tagged_tuple_examples.png" alt="Tagged tuble examples"></p>
<p>So, now I was getting somewhere - I had scraped comments, tokenized the data, and assigned part-of-speech tags to the tokenized data.</p>
<h2 id="step-5-frequency-distributions-of-nouns-and-adjectives">Step 5: Frequency distributions of nouns and adjectives</h2>
<p>For this part of the project, I was interested specifically in nouns and adjectives since they would provide more insight into the tone of the subreddits (as opposed to grammatical, functional words like “the” or “an” that don’t provide much meaning or sentiment in themselves).</p>
<p>The <!-- raw HTML omitted -->nltk<!-- raw HTML omitted --> tag set divides up the general categories of nouns and adjectives into many more sub-categories that reflect more detailed elements such as noun singularity or plurality or whether an adjective is comparative or superlative. You can see the various tags in the table below:</p>
<p><img src="https://codabunga.io/pos_tag_table.png" alt="POS tag table"></p>
<p>For what I was looking for (the top 20 nouns and adjectives in a subreddit), I made a pragmatic decision to lump these sub-categories together into the two general categories of nouns and adjectives. The conversions can be seen in the added third column in the table below.</p>
<p><img src="https://codabunga.io/pos_recat_table.png" alt="POS retag"></p>
<p>In my code, to re-assign the part-of-speech tags to my more generalised categories, I created lists that lumped the more specific tags together (e.g. ‘NN’ or noun singular or ‘NNPS’ or proper noun plural).</p>
<pre><code># PARTS OF SPEECH

# noun singular, noun plural, proper noun singular, proper noun plural

nouns = ['NN', 'NNS', 'NNP', 'NNPS'] 

# adj, adj comparative, adj superlative

adjs = [&quot;JJ&quot;, &quot;JJR&quot;, &quot;JJS&quot;] 
</code></pre>
<p>In doing this, I would still be using the sub-categories of the <!-- raw HTML omitted -->nltk<!-- raw HTML omitted --> library to pick up nouns and adjectives in the comment data, but I would be putting the words into more general part-of-speech lists.</p>
<p>Next, I had to generate a frequency distribution of the nouns or adjectives in the word data for a given subreddit. The frequency distribution would return information on how many times a particular noun or adjective appeared in the data and list this information alongside the counts of the other nouns or adjectives in the sample</p>
<p>I made a function (<!-- raw HTML omitted -->freq_words<!-- raw HTML omitted -->) that used a list comprehension to pull out word forms according to their corresponding part-of-speech tags.</p>
<pre><code># retrieve freqs for nouns &amp; adjectives

def freq_words(community, pos_tag):
return nltk.FreqDist([x[0] for x in community if x[1] in pos_tag and len(x[0]) &gt; 2 and x[0] != &quot;https&quot;])
</code></pre>
<p>In this function, I made sure that the words pulled would be three or more characters long, to filter out punctuation and grammatical words like ‘the’ or ‘an’. I also filtered out the string “https” since this wasn’t a real word but was often found where a user had posted a link to the community. Finally, I selected the surface form of the word to put into my frequency distribution instead of selecting both the surface form and the <!-- raw HTML omitted -->nltk<!-- raw HTML omitted --> tag (e.g. “JJ”). I thought our data plots would be neater without the sub-category tags.</p>
<p>I applied this function to the noun and adjective word data in r/productivity and r/antiwork:</p>
<pre><code># antiwork frequency distribution (nouns/verbs/adjectives)
antiwork_nouns = freq_words(tagged_antiwork, nouns) 
antiwork_adjs = freq_words(tagged_antiwork, adjs)

# productivity frequency distribution (nouns/verbs/adjectives)productivity_nouns = freq_words(tagged_productivity, nouns)
productivity_adjs = freq_words(tagged_productivity, adjs)
</code></pre>
<p>So, now I had frequency distributions for nouns and adjectives for both the r/antiwork and r/productivity comment data. Now it was time for me to visualise the data and find the most common words in the comments.</p>
<h2 id="step-6-plotting-the-top-20-nouns-and-adjectives-for-each-subreddit">Step 6: Plotting the top 20 nouns and adjectives for each subreddit</h2>
<p>Now that I had frequency distributions for the nouns and adjectives in both r/antiwork and r/productivity, it was time to see what the top 20 words were in these subreddits.</p>
<p>I used the <!-- raw HTML omitted -->pandas<!-- raw HTML omitted --> plot function to plot the top 20 nouns and adjectives for each subreddit:</p>
<pre><code># TOP 20 NOUNS 
productivity_nouns.plot(20, cumulative=False, title=&quot;Top 20 r/productivity nouns&quot;)
antiwork_nouns.plot(20, cumulative=False, title=&quot;Top 20 r/antiwork nouns&quot;)

# TOP 20 ADJECTIVES
productivity_adjs.plot(20, cumulative=False, title=&quot;Top 20 r/productivity adjectives&quot;)
antiwork_adjs.plot(20, cumulative=False, title=&quot;Top 20 r/antiwork adjectives&quot;)
</code></pre>
<p>The result of these were a set of graphs that visually plotted the counts of the top 20 words in the subreddit for each part-of-speech as a decreasing frequency distribution.</p>
<p>Let’s have a look at them below!</p>
<h2 id="insight-1-nouns">Insight 1: Nouns</h2>
<p><img src="https://codabunga.io/productivity_nouns_plot.png" alt="Productivity nouns plot">
<img src="https://codabunga.io/antiwork_nouns_plot.png" alt="Antiwork nouns plot"></p>
<p>The top 20 nouns in r/productivity contained topics related to time (‘time’, ‘day’, ‘week’), executive functioning (‘list’, ‘note’/’notes’, ‘habit’, ‘tasks’, ‘calendar’) and objects (‘things’, ‘app’/’apps’, ‘phone’, ‘stuff’). The top 20 nouns in r/antiwork contained more topics related to people (‘people/person’, ‘someone’) and business and societal systems (‘job’/’jobs’, ‘wage’, ‘work’, ‘money’, ‘company’, ‘business’).</p>
<p>Some of the top nouns in this data were related to the concept of time. The top noun lists of r/antiwork and r/productivity both had words related to time. However, the temporal words in r/productivity seemed to relate to an interest in the short-term (&lsquo;day&rsquo;, &lsquo;week&rsquo;) but r/antiwork seemed to relate to an interest in the long-term (&lsquo;years&rsquo;, &lsquo;time&rsquo;, &lsquo;life&rsquo;). You could say that r/productivity and r/antiwork are both interested in time, but r/productivity users focus on the week-by-week perspective while r/antiwork users focus on time on a macro scale.</p>
<p>Another comparison I could make is the use of nouns related to objects as opposed to nouns related to societal systems. The r/productivity data contained more nouns related to objects (e.g. &lsquo;list&rsquo;, &lsquo;phone&rsquo;, &lsquo;stuff&rsquo;, &lsquo;notes&rsquo;), while the r/antiwork data contained more nouns related to people or societal constructs (&lsquo;people&rsquo;, &lsquo;wage&rsquo;, &lsquo;living&rsquo;). This could reflect the tendency of r/productivity users to seek practical tools to complete their work, r/antiwork users to be more interested in critically evaluating the systemic issues that exist in their work in the first place.</p>
<p>Overall, looking at the most common nouns that occur in these two contrasting subreddits can give us some indication of the types of things people tend to talk about in these communities.</p>
<h2 id="insight-2-adjectives">Insight 2: Adjectives</h2>
<p>Looking at adjectives in language data can give us an indication of the emotions of users in a subreddit. Some adjectives connotate positive emotions like joy, excitement, and motivation, while other adjectives can denote negative emotions like anger, resentment, and apathy.</p>
<p><img src="https://codabunga.io/antiwork_adjectives_plot.png" alt="Antiwork adjs plot">
<img src="https://codabunga.io/productivity_adjs_plot.png" alt="Productivity adjs plot"></p>
<p>In both subreddits, the top three adjectives are ‘good’, ‘more’ and ‘other’ (with the order of ‘good’ and ‘more’ swapped in r/antiwork). These are highly frequent words in English that can occur in many contexts. This is a problem if we want to discern the connotation associated with the use of these adjectives. For example, you can use the word “good” to denote a positive emotion, such as “this spaghetti is so good” or “you’re a good egg”. However, you can use the word “good” in sentence constructions with neutral or even negative sentiment, such as “it’s a good indication that things will remain uncertain for a while” or “you’re a good liar, aren’t you?”. This issue arises when using single-word data (as opposed to multiple words or sentences). Overall, it’s hard to discern the connotation associated with adjectives without the surrounding context.</p>
<p>Another issue in this frequency analysis is that the sample size of words is smaller for both subreddits – there simply aren’t as many counts per word as what we can see in noun data. For example, the top adjective ‘more’ in r/antiwork has only 19 counts, whereas the top nouns in both subreddits have over 50 counts.</p>
<p>So, I would think of this adjective data as a fun starting point to think about the sentiment in these subreddits, rather than a reliable indication of it.</p>
<h2 id="data-collection--a-caveat">Data collection – a caveat</h2>
<p>This part of my project was a peep into how words are used in the two subreddits using a little bit of scraped comment data. However, there are flaws with what I’ve done here.</p>
<p>One major flaw is that the <!-- raw HTML omitted -->praw<!-- raw HTML omitted --> function I used to scrape the comment bodies simply dives into the subreddit and scrapes any comment body until that comment count reaches 300. Depending on how many submissions were made that day, and how long each thread was, the sampled comment data can end up representing only two or three subreddit threads. I think this is why some of the data I plotted was weirdly specific (e.g., the word ‘Japanese’ appearing in the top 20 adjectives in r/productivity).</p>
<p>Also, when I ran the script on different days (or even hours), the top 20 words changed each time. Perhaps if I changed the script to only include the top 20 nouns or adjectives from “top comments” rather than any comments (and from “top submissions” rather than any submissions), I could get more generalised data that reflected which words are most favoured in usage in these communities.</p>
<h2 id="summary">Summary</h2>
<p>Overall, I had lots of fun finding the top 20 nouns and adjectives for the two subreddits. It was super cool to practise using some powerful Python libraries (<!-- raw HTML omitted -->praw<!-- raw HTML omitted -->, <!-- raw HTML omitted -->nltk<!-- raw HTML omitted -->, <!-- raw HTML omitted -->pandas<!-- raw HTML omitted -->) and to think about the fascinating reasons behind word choice in two very different Reddit communities.</p>
<p>In terms of limitations, I had to keep in mind that my sample size was small and that my script skews my data collection to specific subreddits. I also couldn’t get much out of the adjective data without seeing the surrounding contexts of those adjectives.</p>
<p>This isn’t the only thing I’m doing with Reddit data by the way – I’m doing ongoing research into Reddit language data (including other subreddits) and I’m excited to keep learning and building!</p>

      </div>
    </article>

    <hr />

    <div class="post-info">
      
      

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text">
          <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
          <line x1="16" y1="13" x2="8" y2="13"></line>
          <line x1="16" y1="17" x2="8" y2="17"></line>
          <polyline points="10 9 9 9 8 9"></polyline>
        </svg>
        2724 Words
      </p>

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
          <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="16" y1="2" x2="16" y2="6"></line>
          <line x1="8" y1="2" x2="8" y2="6"></line>
          <line x1="3" y1="10" x2="21" y2="10"></line>
        </svg>
        
          2022-01-07 11:00
        

         
          
        
      </p>
    </div>
      <hr />
      <div class="sharing-buttons">
        
<a class="resp-sharing-button__link" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcodabunga.io%2fposts%2f2022%2f01%2freddit-and-language-data-part-1-analysing-text%2f" target="_blank" rel="noopener" aria-label="" title="Share on facebook">
  <div class="resp-sharing-button resp-sharing-button--facebook resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M18 2h-3a5 5 0 0 0-5 5v3H7v4h3v8h4v-8h3l1-4h-4V7a1 1 0 0 1 1-1h3z"></path></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://twitter.com/intent/tweet/?url=https%3a%2f%2fcodabunga.io%2fposts%2f2022%2f01%2freddit-and-language-data-part-1-analysing-text%2f" target="_blank" rel="noopener" aria-label="" title="Share on twitter">
  <div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--small">
      <div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://www.tumblr.com/widgets/share/tool?posttype=link&amp;title=Reddit%20and%20language%20data%20part%201%20-%20Analysing%20text&amp;caption=Reddit%20and%20language%20data%20part%201%20-%20Analysing%20text&amp;canonicalUrl=https%3a%2f%2fcodabunga.io%2fposts%2f2022%2f01%2freddit-and-language-data-part-1-analysing-text%2f" target="_blank" rel="noopener" aria-label="" title="Share on tumblr">
  <div class="resp-sharing-button resp-sharing-button--tumblr resp-sharing-button--small">
    <div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14.563 24c-5.093 0-7.031-3.756-7.031-6.411V9.747H5.116V6.648c3.63-1.313 4.512-4.596 4.71-6.469C9.84.051 9.941 0 9.999 0h3.517v6.114h4.801v3.633h-4.82v7.47c.016 1.001.375 2.371 2.207 2.371h.09c.631-.02 1.486-.205 1.936-.419l1.156 3.425c-.436.636-2.4 1.374-4.156 1.404h-.178l.011.002z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="mailto:?subject=Reddit%20and%20language%20data%20part%201%20-%20Analysing%20text&amp;body=https%3a%2f%2fcodabunga.io%2fposts%2f2022%2f01%2freddit-and-language-data-part-1-analysing-text%2f" target="_self" rel="noopener" aria-label="" title="Share via email">
  <div class="resp-sharing-button resp-sharing-button--email resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path><polyline points="22,6 12,13 2,6"></polyline></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://pinterest.com/pin/create/button/?url=https%3a%2f%2fcodabunga.io%2fposts%2f2022%2f01%2freddit-and-language-data-part-1-analysing-text%2f&amp;media=https%3a%2f%2fcodabunga.io%2fposts%2f2022%2f01%2freddit-and-language-data-part-1-analysing-text%2f;description=Reddit%20and%20language%20data%20part%201%20-%20Analysing%20text" target="_blank" rel="noopener" aria-label="" title="Share on pinterest">
  <div class="resp-sharing-button resp-sharing-button--pinterest resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none"><path d="M12.017 0C5.396 0 .029 5.367.029 11.987c0 5.079 3.158 9.417 7.618 11.162-.105-.949-.199-2.403.041-3.439.219-.937 1.406-5.957 1.406-5.957s-.359-.72-.359-1.781c0-1.663.967-2.911 2.168-2.911 1.024 0 1.518.769 1.518 1.688 0 1.029-.653 2.567-.992 3.992-.285 1.193.6 2.165 1.775 2.165 2.128 0 3.768-2.245 3.768-5.487 0-2.861-2.063-4.869-5.008-4.869-3.41 0-5.409 2.562-5.409 5.199 0 1.033.394 2.143.889 2.741.099.12.112.225.085.345-.09.375-.293 1.199-.334 1.363-.053.225-.172.271-.401.165-1.495-.69-2.433-2.878-2.433-4.646 0-3.776 2.748-7.252 7.92-7.252 4.158 0 7.392 2.967 7.392 6.923 0 4.135-2.607 7.462-6.233 7.462-1.214 0-2.354-.629-2.758-1.379l-.749 2.848c-.269 1.045-1.004 2.352-1.498 3.146 1.123.345 2.306.535 3.55.535 6.607 0 11.985-5.365 11.985-11.987C23.97 5.39 18.592.026 11.985.026L12.017 0z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcodabunga.io%2fposts%2f2022%2f01%2freddit-and-language-data-part-1-analysing-text%2f&amp;title=Reddit%20and%20language%20data%20part%201%20-%20Analysing%20text&amp;summary=Reddit%20and%20language%20data%20part%201%20-%20Analysing%20text&amp;source=https%3a%2f%2fcodabunga.io%2fposts%2f2022%2f01%2freddit-and-language-data-part-1-analysing-text%2f" target="_blank" rel="noopener" aria-label="" title="Share on linkedin">
  <div class="resp-sharing-button resp-sharing-button--linkedin resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://reddit.com/submit/?url=https%3a%2f%2fcodabunga.io%2fposts%2f2022%2f01%2freddit-and-language-data-part-1-analysing-text%2f&amp;resubmit=true&amp;title=Reddit%20and%20language%20data%20part%201%20-%20Analysing%20text" target="_blank" rel="noopener" aria-label="" title="Share on reddit">
  <div class="resp-sharing-button resp-sharing-button--reddit resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none"><path d="M12 0A12 12 0 0 0 0 12a12 12 0 0 0 12 12 12 12 0 0 0 12-12A12 12 0 0 0 12 0zm5.01 4.744c.688 0 1.25.561 1.25 1.249a1.25 1.25 0 0 1-2.498.056l-2.597-.547-.8 3.747c1.824.07 3.48.632 4.674 1.488.308-.309.73-.491 1.207-.491.968 0 1.754.786 1.754 1.754 0 .716-.435 1.333-1.01 1.614a3.111 3.111 0 0 1 .042.52c0 2.694-3.13 4.87-7.004 4.87-3.874 0-7.004-2.176-7.004-4.87 0-.183.015-.366.043-.534A1.748 1.748 0 0 1 4.028 12c0-.968.786-1.754 1.754-1.754.463 0 .898.196 1.207.49 1.207-.883 2.878-1.43 4.744-1.487l.885-4.182a.342.342 0 0 1 .14-.197.35.35 0 0 1 .238-.042l2.906.617a1.214 1.214 0 0 1 1.108-.701zM9.25 12C8.561 12 8 12.562 8 13.25c0 .687.561 1.248 1.25 1.248.687 0 1.248-.561 1.248-1.249 0-.688-.561-1.249-1.249-1.249zm5.5 0c-.687 0-1.248.561-1.248 1.25 0 .687.561 1.248 1.249 1.248.688 0 1.249-.561 1.249-1.249 0-.687-.562-1.249-1.25-1.249zm-5.466 3.99a.327.327 0 0 0-.231.094.33.33 0 0 0 0 .463c.842.842 2.484.913 2.961.913.477 0 2.105-.056 2.961-.913a.361.361 0 0 0 .029-.463.33.33 0 0 0-.464 0c-.547.533-1.684.73-2.512.73-.828 0-1.979-.196-2.512-.73a.326.326 0 0 0-.232-.095z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://www.xing.com/app/user?op=share;url=https%3a%2f%2fcodabunga.io%2fposts%2f2022%2f01%2freddit-and-language-data-part-1-analysing-text%2f;title=Reddit%20and%20language%20data%20part%201%20-%20Analysing%20text" target="_blank" rel="noopener" aria-label="" title="Share on xing">
  <div class="resp-sharing-button resp-sharing-button--xing resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none"><path d="M18.188 0c-.517 0-.741.325-.927.66 0 0-7.455 13.224-7.702 13.657.015.024 4.919 9.023 4.919 9.023.17.308.436.66.967.66h3.454c.211 0 .375-.078.463-.22.089-.151.089-.346-.009-.536l-4.879-8.916c-.004-.006-.004-.016 0-.022L22.139.756c.095-.191.097-.387.006-.535C22.056.078 21.894 0 21.686 0h-3.498zM3.648 4.74c-.211 0-.385.074-.473.216-.09.149-.078.339.02.531l2.34 4.05c.004.01.004.016 0 .021L1.86 16.051c-.099.188-.093.381 0 .529.085.142.239.234.45.234h3.461c.518 0 .766-.348.945-.667l3.734-6.609-2.378-4.155c-.172-.315-.434-.659-.962-.659H3.648v.016z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="whatsapp://send?text=Reddit%20and%20language%20data%20part%201%20-%20Analysing%20text%20https%3a%2f%2fcodabunga.io%2fposts%2f2022%2f01%2freddit-and-language-data-part-1-analysing-text%2f" target="_blank" rel="noopener" aria-label="" title="Share on whatsapp">
  <div class="resp-sharing-button resp-sharing-button--whatsapp resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"><path d="M17.472 14.382c-.297-.149-1.758-.867-2.03-.967-.273-.099-.471-.148-.67.15-.197.297-.767.966-.94 1.164-.173.199-.347.223-.644.075-.297-.15-1.255-.463-2.39-1.475-.883-.788-1.48-1.761-1.653-2.059-.173-.297-.018-.458.13-.606.134-.133.298-.347.446-.52.149-.174.198-.298.298-.497.099-.198.05-.371-.025-.52-.075-.149-.669-1.612-.916-2.207-.242-.579-.487-.5-.669-.51-.173-.008-.371-.01-.57-.01-.198 0-.52.074-.792.372-.272.297-1.04 1.016-1.04 2.479 0 1.462 1.065 2.875 1.213 3.074.149.198 2.096 3.2 5.077 4.487.709.306 1.262.489 1.694.625.712.227 1.36.195 1.871.118.571-.085 1.758-.719 2.006-1.413.248-.694.248-1.289.173-1.413-.074-.124-.272-.198-.57-.347m-5.421 7.403h-.004a9.87 9.87 0 01-5.031-1.378l-.361-.214-3.741.982.998-3.648-.235-.374a9.86 9.86 0 01-1.51-5.26c.001-5.45 4.436-9.884 9.888-9.884 2.64 0 5.122 1.03 6.988 2.898a9.825 9.825 0 012.893 6.994c-.003 5.45-4.437 9.884-9.885 9.884m8.413-18.297A11.815 11.815 0 0012.05 0C5.495 0 .16 5.335.157 11.892c0 2.096.547 4.142 1.588 5.945L.057 24l6.305-1.654a11.882 11.882 0 005.683 1.448h.005c6.554 0 11.89-5.335 11.893-11.893a11.821 11.821 0 00-3.48-8.413Z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fcodabunga.io%2fposts%2f2022%2f01%2freddit-and-language-data-part-1-analysing-text%2f&amp;t=Reddit%20and%20language%20data%20part%201%20-%20Analysing%20text" target="_blank" rel="noopener" aria-label="" title="Share on hacker news">
  <div class="resp-sharing-button resp-sharing-button--hackernews resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
			<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none"><path d="M0 24V0h24v24H0zM6.951 5.896l4.112 7.708v5.064h1.583v-4.972l4.148-7.799h-1.749l-2.457 4.875c-.372.745-.688 1.434-.688 1.434s-.297-.708-.651-1.434L8.831 5.896h-1.88z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://telegram.me/share/url?text=Reddit%20and%20language%20data%20part%201%20-%20Analysing%20text&amp;url=https%3a%2f%2fcodabunga.io%2fposts%2f2022%2f01%2freddit-and-language-data-part-1-analysing-text%2f" target="_blank" rel="noopener" aria-label="" title="Share on telegram">
  <div class="resp-sharing-button resp-sharing-button--telegram resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="22" y1="2" x2="11" y2="13"></line><polygon points="22 2 15 22 11 13 2 9 22 2"></polygon></svg>
    </div>
  </div>
</a>

      </div>

    
      <div class="pagination">
        <div class="pagination__title">
          <span class="pagination__title-h"></span>
          <hr />
        </div>

        <div class="pagination__buttons">
          
            <span class="button previous">
              <a href="https://codabunga.io/posts/2022/10/reddit-and-language-data-part-2-generating-text/">
                <span class="button__icon">←</span>
                <span class="button__text">Reddit and language data part 2 - Generating text</span>
              </a>
            </span>
          

          
        </div>
      </div>
    


    

  </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2023</span>
            
                <span><a href="https://codabunga.io">Gina Welsh</a></span>
            
            
                <span><a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></span>
            <span><a href="https://codabunga.io/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a></span>
        </div>
    </div>
    <div class="footer__inner">
        <div class="footer__content">
            
            
          </div>
    </div>
</footer>

            
        </div>

        




<script type="text/javascript" src="https://codabunga.io/bundle.min.dc716e9092c9820b77f96da294d0120aeeb189b5bcea9752309ebea27fd53bbe6b13cffb2aca8ecf32525647ceb7001f76091de4199ac5a3caa432c070247f5b.js" integrity="sha512-3HFukJLJggt3&#43;W2ilNASCu6xibW86pdSMJ6&#43;on/VO75rE8/7KsqOzzJSVkfOtwAfdgkd5BmaxaPKpDLAcCR/Ww=="></script>



    </body>
</html>
