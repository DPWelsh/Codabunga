<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on  </title>
        <link>http://localhost:1313/posts/</link>
        <description>Recent content in Posts on  </description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
        <lastBuildDate>Sat, 29 Jul 2023 19:41:07 +1000</lastBuildDate>
        <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Naive Bayes authorship classification with Facebook Messenger data</title>
            <link>http://localhost:1313/posts/2023/07/naive-bayes-authorship-classification-with-facebook-messenger-data/</link>
            <pubDate>Sat, 29 Jul 2023 19:41:07 +1000</pubDate>
            
            <guid>http://localhost:1313/posts/2023/07/naive-bayes-authorship-classification-with-facebook-messenger-data/</guid>
            <description>1.1 Introduction Many of us have easy access to an incredibly rich, personalised textual dataset.
I have had a Facebook profile since my early high school years. I have access to hundreds of Facebook Messenger conversations to which I have contributed for over a decade. In other words, I have access to an enormous amount of textual data, comprising not only all the written words that I’ve sent over Messenger but also the words that my friends have sent to me.</description>
            <content type="html"><![CDATA[<h4 id="11-introduction">1.1 Introduction</h4>
<p>Many of us have easy access to an incredibly rich, personalised textual dataset.</p>
<p><img src="/fb.png" alt="Facebook"></p>
<p>I have had a Facebook profile since my early high school years. I have access to hundreds of Facebook Messenger conversations to which I have contributed for over a decade. In other words, I have access to an enormous amount of textual data, comprising not only all the written words that I’ve sent over Messenger but also the words that my friends have sent to me.</p>
<p>At the time of writing, Facebook does not delete messages once they are past a certain expiry date, unlike some other messaging apps (e.g., Microsoft Teams). Our pools of conversations can be uncovered easily, years after each data droplet was formed with our keyboards. Sure, it may take some scrolling, but a couple of search terms can very easily bring up the deepest depths of our conversations from some of our most vulnerable and juvenile moments on this Earth.</p>
<p>The existence of this bizarre, superfluous word archive at my fingertips sparked a new project idea. Sure, in a way, it’s awful that so many of the terrible, terrible takes of our pasts are still sitting out there on Facebook servers, perpetually on standby to be retrieved by whoever has the access, the interest, and a link to the conversation in question. So, why wouldn’t I take advantage of this uneasy situation and make a fun learning project out of it?</p>
<p>My thoughts culminated into one focused project idea: an <strong>authorship classifier</strong>. One that is trained on Facebook Messenger data.</p>
<h4 id="12-background-authorship-classification">1.2. Background: Authorship classification</h4>
<p>Classification is a massive part of what we do with our human intelligence. We sort paper mail by postal address, identify people we know on the street by recognising their faces, judge whether or not we thought that the film we just watched was actually good, or just weird in a bad way&hellip;</p>
<p>In a classification process, we gather data about a certain ‘thing’ (which could come from multiple sources) and assign a particular category to it.</p>
<p>As a useful tool, many methods of machine intelligence have been developed for classification tasks. In the realm of natural language processing, text categorisation methods have been developed to assign a label or category to an entire text or document. Some tasks that come to mind include:</p>
<ol>
<li><strong>sentiment analysis</strong> - e.g., analysing whether a review of a book is positive or negative)</li>
<li><strong>spam detection</strong> - filtering out emails that have indicators of being spam)</li>
<li><strong>language identification</strong> – identifying the language in which a text is written</li>
</ol>
<p>The method of authorship classification (also referred to as <strong>authorship attribution</strong>), takes a textual document, processes it, and then categorises it as being written by a particular individual, or author. Such a task has useful applications in the real world – for example, plagiarism detection in the education system, or resolving uncertain authorship of documents in forensic studies.</p>
<p>As is very often the case in computing disciplines, there isn’t just one way to build a tool that does authorship classification. Over the last couple of decades, research scientists and industry developers have studied and applied a range of methodologies for the task. The methods explored can be categorised under a few umbrellas, some of which include:</p>
<ol>
<li><strong>similarity-based models</strong>, which measure how similar an input text is to the textual data collected for that author, e.g., how many words in the text being classified overlap with the words collected for an author in their training data</li>
<li><strong>probabilistic models</strong>, which calculate the probability of each author creating the input string of words</li>
<li><strong>vector-space models</strong> – processing the text being classified by transforming the words into a vector representation</li>
</ol>
<p>For this project, I chose <strong>Naive Bayes Classification</strong> as the model behind my authorship classifier. I thought that this would be a good model to start with for learning purposes, as it is a simple model that can be used to classify many different types of data, is more straightforward to implement, and tends to have solid performance. A great learning source that I used to familiarise myself with the method was the ‘Naive Bayes and Sentiment Classification’ chapter in Jurasfky and Martin’s <a href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing book</a>.</p>
<h4 id="13-naive-bayes-classification-an-explanation">1.3. Naive Bayes Classification: an explanation</h4>
<p>In the context of authorship attribution, the Naive Bayes classification method applies Bayesian inference to calculate the most likely author to have written a given document.</p>
<p>Bayesian inference, which originates from the 18th-century work of Thomas Bayes, works with conditional probabilities. Bayes’ theorem is stated in the mathematical equation:</p>
<p><img src="/naive-bayes-theorem.png" alt="Naive Bayes"></p>
<p>In the right-hand side of the equation above, we’ve got three variables:</p>
<ol>
<li>P(B|A) – a <strong>conditional probability</strong>, that is, the probability of an event B occurring given event A. This is also called the likelihood of event A given a fixed B.</li>
<li>P(A) – the probability of event A, without P(B) in the picture. This is also called the <strong>prior probability</strong>.</li>
<li>P(B) – the probability of event B, without P(A) in the picture. This is also called the <strong>marginal probability</strong>.</li>
</ol>
<p>The above explanation is abstract, so let’s see how it would apply in our context of authorship classification.</p>
<ol>
<li>P(A|B) becomes P(author|document) - that is, the probability of an author being classified given a document being processed.</li>
<li>P(B|A) becomes P(document|author) - that is, the probability of a document being written given the existence of an author.</li>
<li>P(A) – the prior probability of an author. In our context, this would represent the proportion of an author’s writing existing in our training data, out of all the other authors’ writings in the training data.</li>
<li>P(B) – the marginal probability of the document data, in relation to other document data. This variable is actually irrelevant for our classification task – the task is focusing on one document at a time with each run of the classifier. Other than the input text, no other data are are being considered for authorship classification during each run. So, we are conveniently going to drop this variable from our equation.</li>
</ol>
<p>After dropping P(B) from our equation, it reduces to:</p>
<blockquote>
<h5 id="pauthordocument--pdocumentauthor--pauthor">P(author|document) = P(document|author) * P(author)</h5>
</blockquote>
<p><img src="/nb-jurafsky.png" alt="Naive Bayes"></p>
<p>You might be thinking: how are we fitting that entire ‘document’ into our equation here? Wouldn’t we have to break down the document into parts, analysing the different linguistic features inside the document and seeing which features correspond to which author?</p>
<p>And that would be a good question!</p>
<p>The way we are defining our ‘document’ for this task is by simply seeing it as a “bag of words”. The classifier is going to process the entire thing word-by-word and see what vocabulary is used in that document. Then, it will compare that vocabulary with the words that are used by each author in their training data.</p>
<p>The concept of seeing the document as a “bag of words” means that we will consider the presence of each vocabulary item as an individual entity, without considering its co-occurrence with any other vocabulary item or linguistic feature. We know that in linguistic reality, words very much co-occur with one other in very systematic ways, and they don’t often just appear randomly by chance. However, the ‘Naïve’ part of this classification model assumes that each feature (i.e., word) used by an author is an independent event in itself. By assuming independence, you are then able to easily calculate the overall probability of the document occurring by multiplying the chance that an author would use each one of those vocabulary items.</p>
<p>So, we could rewrite our previous equation to be:</p>
<blockquote>
<h5 id="pauthorword1-word2-word3--wordn--pword1--word2--word3--wordn--author--pauthor">P(author|word1, word2, word3, …, wordn) = P(word1 * word2 * word3 * …wordn | author) * P(author)</h5>
</blockquote>
<p>Otherwise stated, we are multiplying the likelihoods of each vocabulary item appearing in the text, regardless of their position within the document. Overall, it’s a very simple way of gauging the probability of an author having written a text – we don’t pay attention to any other linguistic feature other than the presence of certain words.</p>
<p>Upon execution, the classifier runs this Bayesian calculation for each author in the candidate set. It returns the author with the maximum overall posterior probability (i.e., P(author | document) out of all the runs for the authors. This is the output – a classified author!</p>
<p>In a mathematical formula, the concept of returning an author with the maximum posterior probability can be written as such:</p>
<p>In our applied context, it would look a bit more like this:</p>
<blockquote>
<h5 id="run-1-pjoanword1-word2-word3--wordn--pword1--word2--word3--wordn--joan--pjoan--0125">Run 1: P(‘Joan’|word1, word2, word3, …, wordn) = P(word1 * word2 * word3 * …wordn | ‘Joan’) * P(‘Joan’) = 0.125</h5>
</blockquote>
<blockquote>
<h5 id="run-2-pdonword1-word2-word3--wordn--pword1--word2--word3--wordn--don--pdon--0223">Run 2: P(‘Don’|word1, word2, word3, …, wordn) = P(word1 * word2 * word3 * …wordn | ‘Don’) * P(‘Don’) = 0.223</h5>
</blockquote>
<blockquote>
<h5 id="run-3-progerword1-word2-word3--wordn--pword1--word2--word3--wordn--roger--proger--008">Run 3: P(‘Roger’|word1, word2, word3, …, wordn) = P(word1 * word2 * word3 * …wordn | ‘Roger’) * P(‘Roger’) = 0.08</h5>
</blockquote>
<p>In the runs above, Run 2 (Don) has the highest posterior probability returned (0.223), so Don would be returned as our classified author.</p>
<p>For the problem of text classification, it’s very useful to convert the conditional probabilities into log form. The reason for this is that the probabilities of each word being used is often very small (since there are so many words out there) and multiplying each one of them together creates a very, very small number with lots of zeroes. So many zeroes in fact, then when you scale that calculation up, there is a risk of arithmetic underflow! So, log form solves this issue by converting that very small number into a more palatable negative integer. Note that when you work with log numbers, you add them together instead of multiplying them:</p>
<blockquote>
<h5 id="pjoanword1-word2-word3--wordn--logpword1-joan--logpword2-joan--logpword3-joan---logpwordn-joan--logpjoan">P(‘Joan’|word1, word2, word3, …, wordn) = log(P(word1| ‘Joan’) + log(P(word2| ‘Joan’) + log(P(word3| ‘Joan’)), …, + log(P(wordn| ‘Joan’)) + log(P(‘Joan’)).</h5>
</blockquote>
<h3 id="2-implementing-the-classifier">2. Implementing the Classifier</h3>
<p>To implement a Naïve Bayes authorship classifier, there are a number of variables to keep in mind.</p>
<p>The different variables (i.e., parts) of the classifier can be summarised in the following list:</p>
<ol>
<li>A set of <strong>authors</strong> as ‘candidates’ for classification, e.g. [Joan, Don, Roger]</li>
<li>Some <strong>training data</strong>, that is, the balanced set of textual data made of up real text written by each of the candidate authors, to train your classifier on</li>
<li>Some <strong>test data</strong>, that is, a set of textual data collected from the candidate authors that helps you measure how well your classifier is performing after you have trained it on the training data. Here is a rule of thumb that I have seen with allocating training and test data - 80% of your collected author data can be training data, and the remaining 20% can be test data.</li>
<li>A <strong>training function</strong>, that is, a function that trains the classifier on training data</li>
<li>A <strong>applied function</strong>, that is, a function that applies the classifier to test data</li>
</ol>
<p>In my case, I placed the training data in one folder, the test data in another folder, and both the training and testing function in the same Python script.</p>
<h4 id="21-collecting-the-author-candidate-data-from-facebook-messenger">2.1. Collecting the author candidate data from Facebook Messenger</h4>
<p>The very first thing I considered before collecting my friends’ data, for a publicly shared project like this, was to get the <strong>consent</strong> of my selected friends first to use their data in this way.</p>
<p>After all, my friends originally expressed their thoughts with me on Facebook Messenger under the presumption that it would stay within the bounds of our private conversation. So, I would like to reiterate that if you decide to use someone&rsquo;s personal data for a publicly-shared project – be sure to explain to them that you are planning to collect their private data for a personal project, and explain clearly what you would be doing with their data after they give you their permission. This gives them a chance to say yes or no to what you are doing with their data. If you feel like you can’t ask your friend&rsquo;s permission – then don&rsquo;t use their data. Consent is key.</p>
<p>I managed to get the permission of two of my close friends to extract our Facebook Messenger conversations and create ‘textual data pools’ for both of them. I also included my own data in this project.</p>
<p>So, as it stands, there are three authors for my classifier:</p>
<ol>
<li>Amelia</li>
<li>Lewis</li>
<li>Gina (me)</li>
</ol>
<p>To scrape the Facebook Messenger conversation data, I used the <a href="https://github.com/vj-09/FaceBook-Scrape">fbtxtscaper tool</a>. This neat script allows you to log into your Facebook account programmatically, extract an entire conversation history between you and your friend, and store that data in a CSV file. Each row of this CSV file contains messages sent at a certain time by each author.</p>
<p>To work with that CSV file and covert it into textual data ‘pools’ for each author, I wrote some code that would take the CSV file, extract words from each line that is labelled with a certain author (using the <a href="https://docs.python.org/3/library/csv.html">csv</a> module) , and put those words in a text file.</p>
<p>Here is an example of that code applied to author ‘Lewis’:</p>
<pre><code>import csv

csv_data = []

# read CSV file
with open('fb_text_lewis_gina.csv', newline='') as csvfile:
    csv_reader = csv.reader(csvfile)
    for line in csv_reader:
        csv_data.append(line)

# add contents of CSV file to lewis_data.txt
for line in csv_data:
    if line[1] == 'Lewis Ives':
        with open('lewis_data.txt', 'a') as f:
            f.write(line[2] + ' ')
f.close()
</code></pre>
<p>The result is a text file that contains a pool of textual data for that particular author ([author]_data.txt’).</p>
<p>After running that extraction for each of the candidate authors, I had some big, heavy text documents to work with.</p>
<p>My friend Amelia and I had <em>heaps</em> of words in our data sets, which reflects on the fact that we’ve been friends for several years and we have regularly used Facebook to message each other for several years.</p>
<p>On the other hand, my friend Lewis and I have only been friends for just over a year, and we don’t use Facebook Messenger as often to interact with each other, other than mainly to coordinate real-life plans together.</p>
<p>This created an imbalance in the data sets – Lewis’ training dataset only had 10,000 words, whereas Amelia and I had close to 100,000 each. I wanted the data sets for each other to be of equal length, in order to ensure that the training data were balanced and to reduce the chances of the classifier being biased towards a particular author candidate. So, I cut down my and Amelia’s data to 10,000 each, so that the priors for each candidate author – P(author) - could accurately be one third (1/3) each.</p>
<p>In projects that deal with natural language data, it is also important to remember the pre-processing step of <strong>tokenising</strong> the textual data. Tokenising textual data allows the data set to be processed by the classifier as a document of words and word parts, instead of one giant string of random characters.</p>
<p>I used the below code as a function to tokenise the training data documents of each author candidate:</p>
<pre><code>import nltk
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize

def extract_words(file):
    dataset =[]
    with open(file, 'r') as fh:
        dataset = fh.read()
        fh.close()
    word_dataset = word_tokenize(dataset)
    return word_dataset
</code></pre>
<p>I applied this function to each training and test document that I was working with, so that I had a set of variables containing tokenised word data to refer to in my Naive Bayes Classifier script:</p>
<pre><code># training datasets
amelia_document = extract_words('amelia_training_data.txt')
gina_document = extract_words('gina_training_data.txt')
lewis_document = extract_words('./training_data/lewis_training_data.txt')

# test datasets
amelia_test_doc = extract_words('./test_data/amelia_test_document.txt')
gina_test_doc = extract_words('./test_data/gina_test_document.txt')
lewis_test_doc = extract_words('./test_data/lewis_test_document.txt')
</code></pre>
<p>Overall, the steps for data collection for this authorship classifier can be summarised as follows:</p>
<ol>
<li>
<p>Use a web scraping tool that can access and extract entire Facebook Messenger conversations,</p>
</li>
<li>
<p>Separate the conversational data into ‘pools’ for each candidate author,</p>
</li>
<li>
<p>Make sure that the training data is balanced (in number of words) between the candidate authors,</p>
</li>
<li>
<p>Make sure that your training data is tokenised, so that you are working with real words as data, not just strings of characters.</p>
</li>
</ol>
<h4 id="22-feature-selection">2.2. Feature selection</h4>
<p>Before we get started on implementing the classifier itself, I’ll drop a couple of notes on <strong>feature selection</strong>.</p>
<p>The Naive Bayes classification method considers <strong>features</strong> of the data in order to make its class prediction. In our application of the method to text author classification, the classifier considers the presence of vocabulary items within the textual data as features.</p>
<p>A major problem in the realm of text classification is in the <strong>high dimensionality</strong> of the feature space in the domain of textual data. We can attribute the problem to the fact that in human language, many unique words are used to string together paragraphs of expression. In our classifier’s training data, our candidate authors use thousands of unique vocabulary items. Since we are keeping tabs on the conditional probabilities of vocabulary items, this means that our domain has thousands of potential features to work with. Overall, when a classifier has a very high number of features to consider when calculating conditional probabilities, its performance - in terms of speed and even accuracy - can be lowered considerably.</p>
<p>A way of accounting for high dimensionality is to perform feature selection. This process prunes the number of features for the classifier in order to reduce the feature space. When pruning the feature space, we have to make decisions on which features would be most relevant for the classifier to consider.</p>
<p>I narrowed down the set of words in the feature space (i.e. the vocabulary of the classifier)</p>
<pre><code># extract 50 most common words from each author's training data

amelia_freq_dist = [i[0] for i in FreqDist(amelia_document).most_common(50)]
gina_freq_dist = [i[0] for i in FreqDist(gina_document).most_common(50)]
lewis_freq_dist = [i[0] for i in FreqDist(lewis_document).most_common(50)]

# merge vocabulary into set

vocabulary = set(amelia_freq_dist + gina_freq_dist + lewis_freq_dist)
</code></pre>
<p>You can see that I’ve taken the 50 most common words for each author and placed that in a merged set &lsquo;vocabulary&rsquo; as the feature space.</p>
<p>The idea behind this was that I wanted the feature space to contain words that each author is most likely to say, to encapsulate the vocabulary items that are characteristic of each author.</p>
<p>I did try another method of feature extraction. At one point, I made the feature space to be the merged set of &lsquo;unique vocabulary&rsquo; of each author, that is, any vocabulary item in the training data that was written by one author, but not by the other two. The problem with this feature space was that the frequencies of these unique vocabulary items tended to be incredibly low. Often when running the classifier on the test document, the document itself did not contain enough of these unique vocabulary items, to the point that the classifier simply could not get enough information from the text to make a good classification.</p>
<p>So, I went back to making the feature space as the top 50 frequency words for each author – even though a lot of these words would overlap, there would be enough of a sample size for the feature space to contain some less frequent (but distinct) vocabulary items of each author.</p>
<p>Overall, making decisions on the feature space of the classifier can have a big impact on its accuracy and performance.</p>
<h4 id="23-implementing-the-naive-bayes-classifier">2.3. Implementing the Naive Bayes Classifier</h4>
<p>As a place to start, I referred to the pseudocode written out in <a href="https://web.stanford.edu/~jurafsky/slp3/4.pdf">Jurasfky and Martin’s chapter</a> in order to get an idea of how I would start to implement the classifier in Python.</p>
<p>As mentioned above, we have two main parts to the classification script:</p>
<ol>
<li>the training function</li>
<li>the testing function</li>
</ol>
<p>This is how the training function was implemented in my Python script:</p>
<pre><code>def train_naive_bayes(authors, vocabulary):

    all_words = amelia_document + gina_document + lewis_document
        n_documents = len(all_words)
    authorship = {“Amelia”: amelia_document, “Gina”: gina_document, “Lewis”: lewis_document}
    log_priors = {}
    log_likelihoods = {}
    n_authored_data = {}

    for author in authors:
        log_priors[author] = 1/(len(authors))
        n_authored_data[author] = len(authorship[author])
        log_likelihoods[author] = {}
        for unique_word in vocabulary:
            counter = authorship[author].count(unique_word)
            log_likelihoods[author][unique_word] = math.log(counter+1)/(n_authored_data[author]+1*(all_words.count(unique_word)+1/n_documents+1))

    return log_priors, log_likelihoods
</code></pre>
<p>Let’s break down the training function into its component steps:</p>
<ol>
<li>
<p>Load the candidate author set and feature space (i.e. vocabulary) into the train_naive_bayes function</p>
<pre><code> def train_naive_bayes(authors, vocabulary)
</code></pre>
</li>
<li>
<p>Keep track of the content and length of the entire training dataset. Initialise the relation between the candidate author label and their training document, by creating a dictionary ‘authorship’ that links one with the other:</p>
<pre><code> all_words = amelia_document + gina_document + lewis_document
 n_documents = len(all_words)
 authorship = {“Amelia”: amelia_document, “Gina”: gina_document, “Lewis”: lewis_document}
</code></pre>
</li>
<li>
<p>Initialise the dictionaries that record the log priors, log likelihoods and the length of each author’s training document:</p>
<pre><code> log_priors = {}
 log_likelihoods = {}
 n_authored_data = {}
</code></pre>
</li>
<li>
<p>Fill in the above variables by cycling through the candidate author set and creating dictionaries that track the relevant values for each author. For the log_likelihoods variable, I created a nested dictionary, since the conditional probabilities of several vocabulary items will need to be tracked for each author (not just one item):</p>
<pre><code> for author in authors:
     log_priors[author] = 1/(len(authors))
     n_authored_data[author] = len(authorship[author])
     log_likelihoods[author] = {}
</code></pre>
</li>
<li>
<p>Now time for the actual calculation of the conditional probabilities. The script cycles through the input vocabulary (feature space), and counts how many times that vocabulary item appears in each author’s training document.</p>
<pre><code> for unique_item in vocabulary:
     counter = authorship[author].count(unique_word)
</code></pre>
</li>
</ol>
<p>For that vocabulary item, I then took the count of how many times it appears in that training document and added 1 to it (see <a href="https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece">LaPlace smoothing</a>), divided that value by the length of the training document (plus 1), and multiplied that value by the number of times that the vocabulary item appears in all training documents (plus 1), out of all the words that appear in all the training documents (plus 1). Then, convert that whole calculation into its log value, using the math module in Python.</p>
<pre><code>log_likelihoods[author][unique_word]=
math.log((counter+1)/(n_authored_data[author]+1*(all_words.count(unique_word)+1/n_documents+1)))
</code></pre>
<ol start="6">
<li>
<p>Return the log priors and log likelihoods. The output of this function can then be assigned as global variables for the rest of the script to work with.</p>
<pre><code> return log_priors, log_likelihoods
 log_priors, log_likelihoods = train_naive_bayes(authors, vocabulary)
</code></pre>
</li>
</ol>
<h4 id="24-testing">2.4. Testing</h4>
<p>Now that I had the training function implemented, it’s time to actually apply it to some real test data.</p>
<pre><code>def test_naive_bayes(testdoc, logpriors, loglikelihoods, authors, vocabulary):
    sums = {}

    # process test doc
    for author in authors:
        sums[author] = logpriors[author]
        for word in testdoc:
            if word in vocabulary:
                # add author's conditional likelihood of vocab item to sums value
                sums[author] += loglikelihoods[author][word]

    # return author with maximum posterior probability
    for author in sums:
        if sums[author] == max(sums.values()):
            return &quot;Author classified: &quot; + author
</code></pre>
<p>Again, let’s break the test_naive_bayes function into its steps:</p>
<ol>
<li>
<p>Load the test document, log priors and log likelihoods (from the training function), author candidate set and vocabulary into the test function:</p>
<pre><code>def test_naive_bayes(testdoc, logpriors, loglikelihoods, authors, vocabulary):
</code></pre>
</li>
<li>
<p>Create a dictionary called “sums” that will keep track of each author, as well as a counter for how often a vocabulary item appears in the test data. Initialise the value this dictionary as the log prior value for each author (calculated in the training function):</p>
<pre><code>sums = {}
for author in authors:
    sums[author] = logpriors[author]
</code></pre>
</li>
<li>
<p>Cycle through every token in the test document. If that token also appears in the vocabulary list, then add the author’s log likelihood of that word (calculated in the training function) to the author’s value in the “sums” dictionary.</p>
<pre><code> for word in testdoc:
     if word in vocabulary:
         sums[author] += loglikelihoods[author][word]
</code></pre>
</li>
</ol>
<p>The more words in the test document that appear in the vocabulary, and the more that vocabulary appears in the training data of an author, the higher the “sums” value will be for that author – that is, they are more likely to be the author for that test document (based on Naive Bayesian inference):</p>
<p>To explain this calculation a bit further – overall, the classifier is trying to link the frequency of vocabulary items appearing in the test document with the frequency of those items appearing in each author’s training document. For each vocabulary item, the testing function sees how often it appears as a word in the test document. Each time that item is detected in the test document, the author’s “sums” value increments by the conditional likelihood value of that vocabulary item in the author’s training data (<code>loglikelihoods[author][word]</code>).</p>
<p>Take the example of the vocabulary item ‘like’. In my section of the log likelihoods dictionary (author: ‘Gina’), the log probability for this item is -1.2645269104588241. In the the classifier’s test run that calculates my likelihood of being an author candidate, every instance of the word ‘like’ appearing in the test document would increment my “sums” value by -1.2645269104588241 each time.</p>
<p>The word ‘like’ appears 4 times in training document:</p>
<pre><code>sums[‘Gina’] = -0.47712125472 (log prior) + 
(-1.2645269104588241 ) + 
(-1.2645269104588241) + 
(-1.2645269104588241 + 
(-1.2645269104588241)`

&gt; Summed value: -5.53522889656
</code></pre>
<p>Let’s take that same example word ‘like’, and let’s imagine that the log probability was a lower value at -9.134236087786782. Maybe, it would be the result of the word ‘like’ appearing much less in the training document of the author ‘Amelia’:</p>
<pre><code>sums[‘Amelia’] = -0.47712125472 (log prior) + 
(-9.134236087786782) + 
(-9.134236087786782) + 
(-9.134236087786782) + 
(-9.134236087786782)

&gt; Summed value: -37.0140656059
</code></pre>
<p>The word ‘like’ still appears 4 times in this classifier run, since the test document is the same. However, the word contributes way less to the “sums” value of the author Amelia, since her conditional likelihood of this word in her dictionary is much lower.</p>
<p>If the test document contains a high frequency of vocabulary items, and an author’s training data also contains a high frequency of those items, the “sums” value of that author is going to be higher than the others. According to the classifier, they are a more likely candidate to have authored that document, since the vocabulary items have a high rate of co-occurring in both the test document and the author’s training document. On the other hand, if you have a high number of vocabulary items appearing in the test document, but hardly any them appear in the training data of an author, then that particular author’s “sums” value will be lower, making them a less likely candidate to have authored that document.</p>
<p>The last step of the code cycles through the authors in the “sums” dictionary. It returns the author with the highest “sums” value – this is the most likely candidate author, according to the classifier:</p>
<pre><code> for author in sums:
     if sums[author] == max(sums.values()):
        return &quot;Author classified: &quot; + author
</code></pre>
<p>In other words, we have run the test classifier on all the different authors, calculated the “sums” for each one, and will return the author that had the highest value in their “sums” calculation.</p>
<p>To make the results clearer to me, I also wrote a small function that prints out the results neatly to my console:</p>
<pre><code> def print_out_test(doc_string, real_author):
 print(&quot;Real author: &quot; + real_author + &quot; --&gt; &quot; + test_naive_bayes(doc_string, log_priors, log_likelihoods, authors, vocabulary))
 return 
 
 print_out_test(text_file.txt, 'Author')
 &gt; “Real author: 'Author' --&gt; Author classified: 'Author'”
</code></pre>
<p>Overall, the implementation of the training and testing functions did not take up much code at all. I managed to fit it all in to about 30 lines – a testament to the simplicity of the Naive Bayes classification method.</p>
<h3 id="3-results">3. Results</h3>
<h4 id="31-the-output">3.1. The output</h4>
<p>I had a bit of trial-and-error in applying this classifier to the test documents using different methods of feature extraction. My goal was to get it to classify each test document (n words =10,000 each) of the author as the correct one. After much fiddling around (and fixing bugs along the way), I finally got it classifying the correct authors:</p>
<pre><code>print_out_test(amelia_test_doc, &quot;Amelia&quot;)
print_out_test(lewis_test_doc, &quot;Lewis&quot;)
print_out_test(gina_test_doc, &quot;Gina&quot;)

&gt;&gt; “Real author: Amelia --&gt; Author classified: Amelia”
&gt;&gt; “Real author: Lewis --&gt; Author classified: Lewis”
&gt;&gt; “Real author: Gina --&gt; Author classified: Gina”
</code></pre>
<p>Also, a fun thing to do is to put in random strings as input, and get the classifier to guess which one of your friends is most likely to write something like that:</p>
<p>(1) print_out_test(&ldquo;heyo, :^)&rdquo;, &ldquo;N/A&rdquo;)</p>
<pre><code>&gt;&gt; “Real author: N/A --&gt; Author classified: Lewis”
</code></pre>
<p>(2) print_out_test(&ldquo;I&rsquo;m really hungry, really want some choccie&hellip;sad&rdquo;, &ldquo;N/A&rdquo;)</p>
<pre><code>&gt;&gt; “Real author: N/A --&gt; Author classified: Gina”
</code></pre>
<p>I want to emphasise that the results were by no means perfect – even though the whole test documents were correctly classified, failures did happen when I took subsets of training data to test the classifier.</p>
<p>(3) print_out_test(&ldquo;mmm I’ll try to get there like 9 maybe? Ye that is ok! I wanna leave early cos it’s a long drive Cool cool, I’ll message you tomorrow morning I think I’ll be more like 9:30 but I’ll msg you&rdquo;, &ldquo;Amelia&rdquo;)</p>
<pre><code>&gt;&gt; “Real author: Amelia --&gt; Author classified: Gina”
</code></pre>
<h4 id="32-insights">3.2. Insights</h4>
<p>One thing I noticed was that the classifier was very good at correctly classifying one of the authors (Lewis) but found it harder to distinguish between the other two authors (Amelia and Gina).</p>
<p>I think that the classifier’s difficulty in distinguishing between Amelia&rsquo;s and Gina’s (my) data may have had to do with the context in which their training data were collected. The source of these two authors’ training data was the same – their words were extracted from the same conversation between each other. Lewis’ training data, on the other hand, were extracted from a conversation with me, but my data were not extracted from this same source. When I look at my and Amelia’s conversation thread with the naked eye, I can see that we often mirror each other with our language choices, probably due to our close camaraderie and inside jokes. In a way, the collection of our training data would not be so much a representation of our word choices across all domains, but rather a representation of our word choices in relation to when we interact with each other.</p>
<p>So, there’s a sort of asymmetry going on in the domains across the training data. Amelia’s training data is based on Gina-Amelia conversations (but not Lewis-Amelia); Lewis’ training data is based on Lewis-Gina conversations (but not Lewis-Amelia), but Gina’s training data is based on Gina-Amelia conversations (but not Lewis-Gina). I think it would be interesting to see how this classifier would perform if the domain gaps mentioned here were filled.</p>
<p>Or, you could have another kind of textual data – capturing monologues of each author so that you have data that is not expressed in relation to another person, like academic essays. However, you would probably have to test a classifier that is trained on this kind of data with a test document that is also a sort of monologue.</p>
<h3 id="4-conclusion">4. Conclusion</h3>
<p>I found the implementation of the Naive Bayes classification method with Python relatively simple to do. I think it would be cool to experiment with this classification method with other types of features (e.g., character trigrams), other types of training data, and more authors.</p>
<p>Overall, this endeavour was a solid intro to classification methods, and it taught me the basics of Bayesian inference from an applied point of view. I could write the entire classifier script in less than 75 lines. I think it performed well given that it considered just one type of linguistic feature, was trained on a relatively low amount of data, and had a very simple code implementation. It’s nice to have in the toolkit!</p>
]]></content>
        </item>
        
        <item>
            <title>Reddit and language data part 2 - Generating text</title>
            <link>http://localhost:1313/posts/2022/10/reddit-and-language-data-part-2-generating-text/</link>
            <pubDate>Mon, 03 Oct 2022 00:00:00 +0000</pubDate>
            
            <guid>http://localhost:1313/posts/2022/10/reddit-and-language-data-part-2-generating-text/</guid>
            <description>1. Introduction In this post, I’ll show you how I wrote a Python script to generate text using the bigram language model.
For my second project on this site, I wanted to learn to go beyond simply analysing language data on Reddit – I wanted to learn how to generate it.
I thought that a good first foray into language generation would be to take user comment data from a Reddit community (‘subreddit’), create a language model from the linguistic patterns of that data, and then generate new text from those patterns.</description>
            <content type="html"><![CDATA[<h2 id="1-introduction">1. Introduction</h2>
<p>In this post, I’ll show you how I wrote a Python script to generate text using the bigram language model.</p>
<p>For my second project on this site, I wanted to learn to go beyond simply analysing language data on Reddit – I wanted to learn how to generate it.</p>
<p><img src="/Reddit_homepage.png" alt="Reddit Homepage"></p>
<p>I thought that a good first foray into language generation would be to take user comment data from a Reddit community (‘subreddit’), create a language model from the linguistic patterns of that data, and then generate new text from those patterns.</p>
<p>Ideally, I would be able to generate a “comment” from that model – say, twenty to fifty words long - that would mimic a ‘typical’ contribution to that subreddit. I chose two subreddits that seem completely opposed to each other in their users’ belief systems: r/skeptic and r/psychic. I took these two communities as sources of data because I was curious to see how the ethos and communication patterns in these two internet hubs might come out differently as the output of the language model.</p>
<p>So, I established a goal: to write a program to generate strings that mimic the language of commenters on r/skeptic and r/psychic.</p>
<h2 id="2-the-subreddits-skeptics-and-psychics">2. The subreddits: skeptics and psychics</h2>
<p>Let’s look at the characteristics of the r/skeptic and r/psychic subreddits.</p>
<p>The r/skeptic community is built upon the shared goal of ‘generating discussion in the spirit of scientific skepticism.’ The definition they give for this term is ‘the practice of questioning whether claims are supported by empirical research and have reproducibility’. At the time of writing, r/skeptic had just over 163,000 Reddit users subscribed to the forum.</p>
<p><img src="/skeptic-home-page.png" alt="skeptic homepage"></p>
<p>A quick eyeball at the posts of r/skeptic reveals posts that often refer directly to media publications outside of Reddit, such as an anti-vaccine article published on an ‘alternative news’ site. The comment replies to the post are often critical, analytical, and full of quotes or paraphrases from figures of authority, just as you’d expect from a ‘sceptical’ community.</p>
<p>The second subreddit of my project, r/psychic, had a member count of just over 226,000 at the time of writing. It is dedicated to those interested in the belief of ‘extrasensory perception’.</p>
<p><img src="/psychic-homepage.png" alt="psychic subreddit homepage"></p>
<p>The submissions in r/psychic are full of passionate, emotional descriptions of encounters with spirit guides, ghosts, and angels. Overall, the subreddit emphasises self-expression, speculation and validation of the subjective experience in the users’ contributions to the community.</p>
<p>Laying the subreddits side-by-side creates the source of a fascinating range of data across the emotional spectrum. One side of it is careful and analytical, and the other side is excitatory and emotive.</p>
<p>So, I thought it would be fascinating to use language data from these two communities. In this way, I could experiment with the output and create two types of generation based on data from contrasting sources.</p>
<h2 id="3generating-a-sentence-probability-and-word-context">3.	Generating a sentence: probability and word context</h2>
<p>How do you get a computer to generate words?</p>
<p>To get a computer to pick words, you must at least give it a group of possible words to pick from.</p>
<p>Let’s imagine a scenario where a person is sitting with a deck of 200 cards in front of them. On each card, there is a word printed on one side. Some words, such as “can”, “we”, “talk”, appear on more cards than other words do.</p>
<p>The person picks out one card at a time and lays out each card out on a line from left to right.  The player picks out fifty cards one by one, and at the end, she has a line of cards that spell out words – a fifty-word ‘string’. Some words appear much more often than other words do. For example, the word “the” appears fifteen times in the fifty-word string, the pronoun “you” appears seven times, “think” appears five times, and “would” appears twice. The fifty-word string she picks out (i.e., “generates”) ends up being nonsensical:</p>
<p><img src="/unigram-example.png" alt="unigram example"></p>
<p>The 200-card deck (i.e., the ‘language model’) had the cards for the player to pick from, but it didn’t have any context for which cards make more sense when they appear alongside each other when the player puts them down. So, the player must be able to predict which card is most likely to come next, given the cards that have just been put down.</p>
<p>The problem above reflects the fact that in human languages, not all words are equally likely to appear next to one another. You have certain words that are more likely to appear close to one another than others. Take for example the following sentence:</p>
<pre><code>Could you please turn the light …
</code></pre>
<p>Which word would you choose after the word ‘light’?</p>
<p>The most likely contenders would be the prepositions ‘on’ or ‘off’, both of which are plausible options. What about less likely contenders? A native speaker of standard English would be unlikely to insert the words ‘under’, ‘beneath’, or ‘horse’ as the next word in this sentence.</p>
<p>So, a basic language model that predicts the next word in a sentence must go beyond the simple act of randomly picking words from a pre-defined list. It must be able to assign the probability of a word appearing as the next possible word given how a sentence has been constructed thus far.</p>
<h2 id="4bigrams-calculating-the-probability-of-a-sentence">4.	Bigrams: calculating the probability of a sentence</h2>
<p>To generate a sentence, you need to be able to predict the next word based on the probability that a particular word would occur. There are a few ways of going about this. However, a simple model to start with is the bigram language model.</p>
<p>The bigram language model calculates probabilities of each word occurring in a sentence, based on the probability of that particular word occurring after the previous word in the sentence. It segregates sentences into pairs of words that occur next to each other, and makes calculations based on these pairing patterns.</p>
<p>Let’s take the full sentence example: “Could you please turn the light off”. If that partial sentence were divided up into adjacent word pairs, it would be divided into six sections, like below:</p>
<pre><code>1)	(“Could”, “you”)

2)	(“you”, “please”)

3)	(“please”, “turn”)

4)	(“turn”, “the”)

5)	(“the”, “light”)

6)	(“light”, “off”)
</code></pre>
<p>Each of these sections could be assigned probabilities. That is, each word pair (bigram) could be given a probability of occurring based on how often that word pairing occurs in a language, out of all the word pairings that ever occur in a language. In the table below, I assigned some made-up probabilities to each bigram from “Could you please turn the light off” to demonstrate how bigrams could get assigned probabilities based on how frequently they occur in a language.</p>
<pre><code>Bigram	            (Made-up) probability
(“could”, “you”)	0.05
(“you”, “please”)	0.00002
(“please”,”turn”)	0.000012
(“turn”, “the”)	    0.003
(“the”, “light”)	0.005
(“light”, “off”)	0.0003
</code></pre>
<p>In my made-up examples, I assigned “could you” as having a higher probability than “please turn”, which would mean that “could you” occurs more in speech than “please turn”. As a result, it would be more likely to be picked out as a candidate for language generation model. So, we keep a count of how often bigrams occur in language data to indicate which words are more likely to occur together.</p>
<p>How would we know how often a particular word pairing occurs in a language? Well, we can’t know all the possible word pairings that ever occur among speakers of a language at any given time. However, we can approximate the frequency of bigrams in a language if we work with a good language corpus. A language corpus is a collection of language data for a given language, often organised around a particular domain. For example, an Australian English language corpus could contain transcripts of all television media produced between 1970 and 1990. A linguistic researcher could have a conversational corpus of Navajo language child speakers socialising with other kids. A corpus often has a sizeable number of words that give a representative view of a particular language or dialect, within a particular domain, at a particular time.</p>
<p>By extracting natural language data from a language corpus, you can start creating plausible probabilities of bigrams occurring within a certain language, in a certain domain.</p>
<p>The table below shows how bigrams and word frequencies relate. The frequencies of specific words occurring adjacent to other words are set out along the x- and y-axis.</p>
<p><img src="/9222_sentences.png" alt="bigram table example"></p>
<p>You can see that in the bigram data, certain words occur much more frequently than others. Out of the sample of 9222 sentences taken from a British telephone conversation data corpus, “I” is paired with “want” 827 times, “to” and “eat” 686 times, “to” and “spend” 211 times, and “Chinese food” 82 times. Using this data, you can assign probabilities based on the number of times those bigrams appear out of all the possible bigrams in the language corpus data. In the example above, quite a few bigrams never occur. For instance, “eat I” doesn’t occur in the sample at all, and neither does “spend want”, “lunch Chinese”, nor “eat want”. These bigrams are assigned probabilities of 0, in the crude way we&rsquo;re defining here. In the real world, they would not be assigned probabilities of 0 – check out <a href="https://technewsiit.com/laplace-smoothing-and-naive-bayes-algorithm">Laplace smoothing</a> if you’re curious as to why.</p>
<p>These bigram probabilities can be used in language generation to pick out what the next word would likely be in a sentence, given the last word that has been generated in the sequence so far.</p>
<pre><code>“I”
“I want”
“I want to”
“I want to eat”
“I want to eat Chinese”
“I want to eat Chinese food”
</code></pre>
<p>Let’s have a look at how you could implement the bigram model using Python and the Natural Language Toolkit (NLTK) package.</p>
<h2 id="5putting-it-all-together-generating-strings-with-python-and-nltk">5.	Putting it all together: generating strings with Python and NLTK</h2>
<h3 id="51-scraping-the-comments">5.1. Scraping the comments</h3>
<p>The first step in creating the comment generator was to create two “corpora” – one language corpus for r/skeptic, and another for r/psychic. These would act as data batches from which the frequencies of bigrams would be derived. From those frequencies, the bigram language model can be built to assign probabilities to words occurring next to one another in pairs. For this step, I would have to connect to the two subreddits directly and scrape comment data from the users contributing to these forums.</p>
<p>I used the <!-- raw HTML omitted -->praw<!-- raw HTML omitted --> API to connect to the two subreddits. For this step, I had to <a href="https://www.reddit.com/login/?dest=https%3A%2F%2Fwww.reddit.com%2Fprefs%2Fapps%2F">register an application on Reddit</a>. Once this was set up, I used the following code to authorise my Python script to programmatically access the Reddit site. Note that the <!-- raw HTML omitted -->client_id<!-- raw HTML omitted -->, <!-- raw HTML omitted -->client_secret<!-- raw HTML omitted -->, and <!-- raw HTML omitted -->user_agent<!-- raw HTML omitted --> parameters would be different for someone else connecting to Reddit, so I’ve just placed <!-- raw HTML omitted -->“XXXXXX”<!-- raw HTML omitted --> in the values for these parameters.</p>
<pre><code>import praw
from praw.models import MoreComments

# connect to reddit

reddit = praw.Reddit(client_id='XXXXXX’,
                    client_secret='XXXXXXX’,
                    user_agent=’XXXXXX’)
</code></pre>
<p>Next, I created separate ‘psychic’ and ‘skeptic’ subreddit data objects. These objects would contain corpus data from the two subreddits.</p>
<pre><code>psychic = reddit.subreddit('psychic')
skeptic = reddit.subreddit('skeptic')
</code></pre>
<p>Then, I wrote a function <!-- raw HTML omitted -->scrape_comments<!-- raw HTML omitted --> that would trawl through the comments of the newest 200 submissions of the two subreddits (that is, newest at the time the script is run). Then, the function would store these comments to two text files for each subreddit, acting as data batches for the two subreddits.</p>
<pre><code>def scrape_comments(sub):
    for submission in sub.new(limit=200):
        for comment in submission.comments:
            if isinstance(comment.body, MoreComments):
                continue
            with open(f&quot;{sub}_data_store.txt&quot;, 'a') as fh:
                fh.write(comment.body + ' ')
</code></pre>
<p>I applied the function to the r/psychic and r/skeptic data objects so that I would have two data batches to pull from.</p>
<pre><code>scrape_comments(psychic)
scrape_comments(skeptic)
</code></pre>
<p>I ran the scripts many times to build up each corpus text file to contain 20,000 words from r/skeptic and r/psychic, respectively.</p>
<p><img src="/skeptic-data-store-text.png" alt="screenshot of skeptic data store"></p>
<p>Above is a screenshot of the first several lines of the r/skeptic corpus text file.</p>
<h3 id="52-gathering-frequency-values">5.2 Gathering frequency values</h3>
<p>Now that I had substantial data batches (“corpora”) for the two subreddits, it was time to organise the language data into bigrams and frequency distributions. This would enable the bigram language model to calculate the probabilities of different bigrams appearing in the comment data. For this task, I used the <!-- raw HTML omitted -->nltk<!-- raw HTML omitted --> modules <!-- raw HTML omitted -->ngrams<!-- raw HTML omitted -->, <!-- raw HTML omitted -->word_tokenize<!-- raw HTML omitted -->, <!-- raw HTML omitted -->sent_tokenize<!-- raw HTML omitted --> and <!-- raw HTML omitted -->FreqDist<!-- raw HTML omitted -->. I also imported the <!-- raw HTML omitted -->scrape_data<!-- raw HTML omitted --> function from my earlier comment-scraping script.</p>
<pre><code>import random
from nltk import ngrams
from nltk import word_tokenize
from nltk import sent_tokenize
from nltk import FreqDist
from scrape_data import reddit
from scrape_data import psychic
from scrape_data import skeptic
</code></pre>
<p>Next, I wrote a function <!-- raw HTML omitted -->collect_data<!-- raw HTML omitted --> that would take the data from the batch files and created test data objects, <!-- raw HTML omitted -->test_data_psychic<!-- raw HTML omitted --> and <!-- raw HTML omitted -->test_data_skeptic<!-- raw HTML omitted -->, to pull data from the text files of the two corpora.</p>
<pre><code>def collect_data(sub):
    data_collection = []
    with open(f&quot;{sub}_data_store.txt&quot;, 'r') as fh:
        read_file = fh.readlines()
        for i in read_file:
            data_collection.append(i)
    return data_collection

test_data_psychic = collect_data(psychic)
test_data_skeptic = collect_data(skeptic)
</code></pre>
<p>At this point, we have lots of comment strings as data in our r/skeptic and r/psychic stores. However, we cannot linguistically analyse these strings without applying the tokenization method to them. Tokenizing our comment data will allow our script to process the data as separate words and sentences, rather than a random array of characters. It’s an essential step to working with bigrams, as the program would be able to recognize words within a string, and therefore process them as separate entities next to one another.</p>
<p>In my script, I applied a word tokenization function from <!-- raw HTML omitted -->nltk<!-- raw HTML omitted --> (<!-- raw HTML omitted -->word_tokenize<!-- raw HTML omitted -->) as well as their sentence tokenisation function (<!-- raw HTML omitted -->sent_tokenize<!-- raw HTML omitted -->) to both the r/skeptic and r/psychic data. I applied both forms of tokenization so that the script would be able to recognize separate words as well as the start and end of each sentence in the comment data.</p>
<pre><code>skeptic_tokens = [word_tokenize(w) for w in sent_tokenize(str(test_data_skeptic))]

psychic_tokens = [word_tokenize(w) for w in sent_tokenize(str(test_data_psychic))]
</code></pre>
<p>Now, it was time to create the bigrams!</p>
<p>I defined a function <!-- raw HTML omitted -->get_bigrams<!-- raw HTML omitted --> that would create a list of bigrams with padding symbols to indicate the start (<!-- raw HTML omitted -->&quot;&lt;s&gt;&quot;<!-- raw HTML omitted -->) and the end (<!-- raw HTML omitted -->&quot;&lt;/s&gt;&quot;<!-- raw HTML omitted -->) of a sentence. I wanted to make these symbols overt in the data, so that it was easy to pick out the most common first and last words of sentences in the language data. This way, the model would be better at guessing realistic first and last words while generating a sentence.</p>
<pre><code>def get_bigrams(token_data):
    bigrams = [list(ngrams(i, 2, pad_left=True, pad_right=True,
                        left_pad_symbol='&lt;s&gt;', right_pad_symbol='&lt;/s&gt;')) for i in token_data]
    output = [j for i in bigrams for j in i]
    return output

bigrams_psychic = get_bigrams(psychic_tokens)
bigrams_skeptic = get_bigrams(skeptic_tokens)
</code></pre>
<p>Next, I created a frequency distribution of bigrams occurring in the r/skeptic and r/psychic corpus data. In this way, I could build up the probabilities of co-occurring words in the language data.</p>
<pre><code>freq_psychic = FreqDist(filter_bigrams(bigrams_psychic))
freq_skeptic = FreqDist(filter_bigrams(bigrams_skeptic))
</code></pre>
<p>Lastly, I wanted a separate list of any words that occur as the first word of a sentence in the data. If a word occurred immediately after the start padding symbol, then the word’s bigram was added to this list.</p>
<pre><code># filter out start tokens in frequency data

def start_tokens_lst(frequency_data):
    return [i for i in frequency_data if i[0] == '&lt;s&gt;']

# create starting token list for r/psychic

starting_tokens_psychic = start_tokens_lst(freq_psychic)

# create starting token list for r/skeptic
starting_tokens_skeptic = start_tokens_lst(freq_skeptic)
</code></pre>
<p>So, I had the frequencies of each bigram in the language data for both subreddits as well as a separate list of ‘start’ words in the data that occur as the first word of any sentences. These start words were extracted from bigrams with the first item of the bigram being the start word token. The second item of this bigram would be a word that occurs at the start of at least one sentence in the corpus data.</p>
<pre><code>[('&lt;s&gt;', 'If'), ('&lt;s&gt;', 'Brilliant'), ('&lt;s&gt;', 'This'), ('&lt;s&gt;', 'They'), ('&lt;s&gt;', 'I'), ('&lt;s&gt;', 'From'), ('&lt;s&gt;', 'Hypotheses'), ('&lt;s&gt;', 'Nice'), ('&lt;s&gt;', 'Simple'), ('&lt;s&gt;', 'As')]
</code></pre>
<p>With this kind of information, I could get started on creating the function that actually generates sentences from an empty string.</p>
<h2 id="53-generating-the-comments">5.3. Generating the comments</h2>
<p>I wrote a function <!-- raw HTML omitted -->generate_start_token<!-- raw HTML omitted --> that uses the in-built Python function <!-- raw HTML omitted -->random.choice()<!-- raw HTML omitted -->. This function randomly picks out a word from the list of ‘start’ words that occur in the language corpus. The word randomly picked would be (suitably) the first word in my generated sentence.</p>
<pre><code># return random starting token from starting token list (as first word)
def generate_start_token(starting_tokens):
return random.choice(starting_tokens)
</code></pre>
<p>Then, I defined a list of frequency values (values_lst), which is a list of the actual numbers of times that each bigram appears in the corpus, not the bigrams themselves.</p>
<pre><code># list of frequency values from freq bi list

# values_lst = [i for i in freq_bi.values()]
</code></pre>
<p>Now, it was time to create the main function, which generates a string of twenty words.</p>
<pre><code>def generate_string_by_freq(freq_bi, starting_tokens):
    start = generate_start_token(starting_tokens)
    string = start
    last_word = start
    counter = 20
    while counter &gt; 0:
        items_lst = [freq_bi[i] for i in freq_bi if i[0] == last_word[1]]
        max_value = max(items_lst)
        lst = [i for i in freq_bi if i[0] == last_word[1]]
        if counter % 2 == 0:
                append_item = random.choice([i for i in freq_bi if i[0] == last_word[1] and freq_bi[i] == max_value])
        else:
            append_item = random.choice([i for i in freq_bi if i[0] == last_word[1]])
        string = string + append_item
        if append_item[1] == '&lt;/s&gt;':
            update = random.choice(starting_tokens)
        else:
            last_word = append_item
        counter = counter - 1
    output = list(string)
    return &quot; &quot;.join(output[1:][::2])
</code></pre>
<p>Let’s break this one down.</p>
<pre><code>start = generate_start_token(starting_tokens)
string = start
last_word = start
</code></pre>
<p>First, I had a start word randomly picked using the generate_start_token function. This would give us our first word in the sentence. Then, I created a variable <!-- raw HTML omitted -->string<!-- raw HTML omitted --> that assigned that first word as its value. I also created that first word as the value of a variable <!-- raw HTML omitted -->last_word<!-- raw HTML omitted -->. The ‘last_word’ variable is the last word that has been assigned to the string within the function loop. This will keep being updated as more words get added to the string.</p>
<p>Next, I assigned a counter to the function that dictates how many words the script would add to the string. I set the initial counter value at twenty, so that the number of words in my string would be twenty.</p>
<pre><code>counter = 20
</code></pre>
<p>Then, I created a while loop that would use this counter function to generate the twenty words in the sentence. A condition was placed on the way the words would be picked out from the corpus data.</p>
<pre><code>while counter &gt; 0:
        items_lst = [freq_bi[i] for i in freq_bi if i[0] == last_word[1]]
        max_value = max(items_lst)
        if counter % 2 == 0:
            append_item = random.choice([i for i in freq_bi if i[0] == last_word[1] and freq_bi[i] == max_value])
        else:
            append_item = random.choice([i for i in freq_bi if i[0] == last_word[1]])
</code></pre>
<p>If the counter was sitting on an odd number, then the next word would be chosen from the corpus data if, in the data, it occurs immediately after the word that has just been added to the string. If the counter was sitting on an even number, then the next word would be chosen from the corpus data if it occurs immediately after the last word added to the string AND it is the word that occurs most frequently in that particular position, that is, immediately after the previous word.</p>
<p>I chose this method because I wanted the generated comments to reflect highly frequent words in the data, as well as less frequent words occurring in the data. In real life, speakers of a language don’t always choose the most frequent words, especially if it’s in a context of social expression or critique – instead, they are likely to choose a combination of highly frequent words and less frequent words. So, my counter loop would have half the words chosen based on the maximum probability of that word’s position, and the other half chosen randomly.</p>
<p>Each time this loop would run, I would append the bigram of the newly generated word to the ‘string’ tuple (string = string + append_item), which would help build up the sentence string. Then, I set a conditional for the ‘last_word’ variable. If the last generated word occurs before an end token symbol, indicating that it occurs as the last word of a sentence, then I would generate a new start word as the <!-- raw HTML omitted -->last_word<!-- raw HTML omitted -->. Doing so would start a new sentence for the next part of the comment string. If the most recently generated word does not occur before an end token, then the ‘last_word’ would be updated to be its bigram. In this way, I could make a comment string that has several sentences.</p>
<pre><code>string = string + append_item
if append_item[1] == '&lt;/s&gt;':
last_word = random.choice(starting_tokens)
else:
last_word = append_item
</code></pre>
<p>No matter what the outcome of the loop was, each time it was called, it would decrement the counter by one. When the counter reached zero then the looping would terminate.</p>
<pre><code>counter = counter – 1
</code></pre>
<p>The final lines of the program put the chosen bigrams into a list and returned the output of the function in a readable string format.</p>
<pre><code>output = list(string)
return &quot; &quot;.join(output[::2])
</code></pre>
<p>Let’s have a look at some of the results in the next section.</p>
<h2 id="6-results">6. Results</h2>
<p>To communicate the results of the bigram model, let me first show you what the generation looked like with a unigram model.</p>
<p>In a unigram model, each word in a sentence is assumed to occur independently of any other words surrounding it. So, the probability of a word’s occurrence is not calculated based on any words that have preceded it. As we know now, in a bigram model, the calculation of a word probability considers the context of the word immediately preceding it.</p>
<p>I tested out what the results would look like with just a unigram model. Here is a sample of that from the r/skeptic data:</p>
<pre><code>All guy used enough prized got the fall test wrong way we I the 1,700 are to to be some what have the way our health evidence do despair possible 18 is who 'm up broken years ' get part dealing this people spend readers some -- to mention and respond problem going would say he blocked horrible way crunchy understand So right are had when bias that too approach I ve thinking ’ arent by young the brain people demonstrable they valves narratives bring under they a in experience wonder though
</code></pre>
<p>The text in the output of the unigram model is very strange. It shows some common words that do occur in the subreddit data, but that is where it stops. The words do not coalesce smoothly. There are word repetitions that would never occur in natural speech, like “to to”. As a result, the unigram language model shows little semblance to what a comment would look like in the reddit subcommunities.</p>
<p>Afterwards, I tested out the results with the bigram model and after implementing the generation function ‘generate_string_by_freq’ outlined in section 5. The results of that are listed in the table below, containing five examples from the two subreddits. The data from the two r/psychic and r/skeptic corpora were put through the same function, but they generated different ‘moods’ based on the language that real commenters use in the communities.</p>
<p><img src="/psychic-skeptic-output-table.png" alt="table comparing results of psychic and skeptic"></p>
<p>The bigram output is easier to read than the unigram output because word context is now accounted for. However, there are some other elements at play that are preventing the generated comments from looking realistic.</p>
<p>The main limitation is in the way the generated comments flow on the sentence level. If you segmented the comments into sections of two words each, each section would make sense. Take for example the generated sentence “Vote for the Church covered, and Harris where nutters are eating like I&rsquo;d be removed Stores that classic CJD”. If you split this sentence up into sections of word pairs, each word pair looks realistic: “Vote for”, “the Church”, “are eating”, “I’d be”, et cetera. However, when you string the entire sentence together, it does not flow smoothly at all, and it makes little sense.</p>
<p>The bigram data has helped with some of the comments’ realism, in that the grammaticality between words is kept intact in a way that the unigram model output does not demonstrate. For example, the instances of strings like “we are a lot like a circle pointing”, “the supporters of countries willing to be too much”, and “that losers are normally” all adhere to English-like patterns, such as subject-verb agreement (“losers are”, “we are”) and the use of nouns after prepositions (&ldquo;of countries&rdquo;). However, the lack of comprehensibility on the level of the entire sentence shows the limitations of implementing just a basic bigram model for language generation. Without the extra methods that would unify the words beyond their relation to the immediately preceding word, it does not look like a &lsquo;true&rsquo; comment in the holistic sense.</p>
<p>That being said, the comments show some mimicry of user comments in the two subreddits. It gives some clues as the overall sentiment of contributions to the communities. Let’s pick out a specific contrasting pair:</p>
<p><img src="/psychic-skeptic-one-example.png" alt="psychic vs skeptic table one example"></p>
<p>Many of the r/psychic comment strings focus on subjective experience, a pattern indicated by frequently generated 1st person pronouns (‘I’, ‘we’, ‘my’) alongside nouns that focus on personal experience and emotion (‘discovery’, ‘attachment’). On the other hand, the comment strings generated with the r/skeptic data employ more 3rd person pronouns and referents (‘they’, ‘kids’), indicating that the users are often talking about groups outside of themselves as subject matter. I also noticed that the r/skeptic comments made many more references to specific world events, such as COVID-19, the AIDS crisis, US elections, or anti-vaxxer protests. In contrast, the r/psychic comments made less reference to particular global incidents or points of history. Instead, the subject matter was often of a more generalised spiritual nature, such as the concepts of ‘déjà vu’, ‘manifestation’, or ‘free will’.  These differences in subject matter reflect the dissimilarities of the two subreddits – one is focused on personal and generalised human experiences, whereas the other aims to break down specific global events while striving for a ‘third-person’ lens.</p>
<h2 id="7conclusion">7.	Conclusion</h2>
<p>I would describe my first experiences in language generation to be somewhat successful. However, there are certainly a lot of limitations to the method I used. I would say that the text that my script generated did reflect the general mood of comments from both communities. But, while the comments made sense on the two-word level, it often wasn’t fully comprehensible on the sentence level. So, there was still a lot of room for improvement to make the comments more realistic looking.</p>
<p>Overall, I found this project to be a great learning experience in encountering the challenges of language generation. I would be fascinated to learn how to make generated text comprehensible on the sentence level. It would also be cool to find out about other methods to generate the probabilities of words (or sentences) occurring within a language.</p>
<p>For my next project on this site, I think I’ll take a break from the Reddit data stuff and focus on a different interest. I have a friend who wants to do some of her own research into the ways people use a particular type of sentence in English. I offered to help her out on that by getting useful data from massive English corpora. So, stay tuned, I could be inspired by that!</p>
]]></content>
        </item>
        
        <item>
            <title>Reddit and language data part 1 - Analysing text</title>
            <link>http://localhost:1313/posts/2022/01/reddit-and-language-data-part-1-analysing-text/</link>
            <pubDate>Fri, 07 Jan 2022 00:00:00 +0000</pubDate>
            
            <guid>http://localhost:1313/posts/2022/01/reddit-and-language-data-part-1-analysing-text/</guid>
            <description>Introduction I’m barely on Reddit these days.
However, when I was on Reddit, I found it interesting how different Reddit communities (‘subreddits’) seemed to have distinct flavours in the way users socially contribute to their forum.
Since Reddit is a host for a diverse group of communities banded around interests, it would follow that the type of language used in each subreddit would have its own special signature of popular word choices, sentiments, and social dynamics.</description>
            <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>I’m barely on Reddit these days.</p>
<p>However, when I was on Reddit, I found it interesting how different Reddit communities (‘subreddits’) seemed to have distinct flavours in the way users socially contribute to their forum.</p>
<p><img src="/Reddit_homepage.png" alt="Reddit Homepage"></p>
<p>Since Reddit is a host for a diverse group of communities banded around interests, it would follow that the type of language used in each subreddit would have its own special signature of popular word choices, sentiments, and social dynamics.</p>
<p>I knew there would be fascinating ways to analyse and compare the language data of different subreddits. So, I decided to start a project where I’d scrape data from different subreddits and do some linguistic analysis on the comments. I was most interested in comparing the differences between selected subreddits.</p>
<p>For this post, I will make comparisons of word data between two subreddits:</p>
<ol>
<li>r/productivity, a subreddit that “shares tips and tricks for being more productive”, and</li>
<li>r/antiwork, a subreddit where users are “curious about ending work” and “want to get the most out of a work-free life”</li>
</ol>
<p>I chose these two subreddits for this post since they both relate to the topic of work, but they approach the topic from very different perspectives. They are also both significant communities on Reddit, with r/productivity having almost 900,000 members at the time of writing and r/antiwork having 1.5 million “idlers”. So, I made a bet that contrasting the language used in these two communities could lead to some interesting insights.</p>
<p>To start this project, I kept it very simple. My first task focused on word frequency analysis, that is, finding insights from how often certain words appear in a sample of data (e.g. the word “cup” appears 5 times in a sample paragraph).</p>
<p>In this post, I will explore how I obtained the top 20 nouns and adjectives in r/productivity and r/antiwork. Towards the end, I do a little bit of armchair analysis into why these top nouns and adjectives might occur in the sampled data.</p>
<h2 id="step-1-setting-up-the-required-python-libraries">Step 1: Setting up the required Python libraries</h2>
<p>For this step, I will presume that you’ve already got a version of Python 3 and whatever IDE you use on your computer. You’ll also need to install <!-- raw HTML omitted -->nltk<!-- raw HTML omitted --> and <!-- raw HTML omitted -->pandas<!-- raw HTML omitted --> Python libraries as well as the <!-- raw HTML omitted -->praw<!-- raw HTML omitted --> API, if you haven’t already - you can find links to each respectively <a href="https://www.nltk.org/">here</a>, <a href="https://pandas.pydata.org/docs/">here</a> and <a href="https://praw.readthedocs.io/en/stable/index.html">here</a>.</p>
<p>At the top of my script, I imported the <!-- raw HTML omitted -->nltk<!-- raw HTML omitted -->, <!-- raw HTML omitted -->praw<!-- raw HTML omitted --> and <!-- raw HTML omitted -->pandas<!-- raw HTML omitted --> libraries since I’d need these later in the script. For the <!-- raw HTML omitted -->nltk<!-- raw HTML omitted --> library, I made sure to specifically import the word_tokenize and pos_tag modules, since we’ll be needing this to analyse the words in the language data.</p>
<pre><code>import nltk
from nltk import word_tokenize
import nltk.data
from nltk.probability import FreqDist
from nltk.tag import pos_tag
import praw
import pandas as pd
</code></pre>
<h2 id="step-2-accessing-the-subreddits">Step 2: Accessing the subreddits</h2>
<p>The PRAW API was the element that connected my Python script to the actual Reddit data itself. This is the part that ‘scrapes’ the real data. For this step, I had to first <a href="https://www.reddit.com/prefs/apps/">register an application on Reddit</a>. Once this was set up, I used the following code to authenticate my script to access Reddit (the ‘XXX’ details in this code will depend on the user details of your reddit application):</p>
<pre><code># access reddit
reddit = praw.Reddit(client_id='XXX', 
                 client_secret='XXX', \
                 user_agent='XXX')
</code></pre>
<p>Next, I had to assign variables that accessed specific subreddits of r/antiwork and r/productivity. I used the reddit.subreddit() function, putting the subreddits (‘antiwork’ and ‘productivity’) into each parameter:</p>
<pre><code># assign initalising variables to the four subreddits - connect to them via reddit API
antiwork = reddit.subreddit('antiwork')
productivity = reddit.subreddit('productivity')
</code></pre>
<p>This connected my script to the r/antiwork and r/productivity subreddits.</p>
<p>Now time for the real fun!</p>
<h2 id="step-3-collecting-the-data">Step 3: Collecting the data</h2>
<p>Now that I had access to the subreddits, a question popped into my head. What type of data did I want to scrape from these communities?</p>
<p>Submissions to Reddit come in various data formats such as text, images, links, and videos. When I looked at the subreddits with my naked (human) eye, I noticed that a lot of submissions (the contributions created by ‘original poster’ users) were in the image data format. This kind of format was useless for what I was trying to do with my project since I needed actual text to make sense of the data linguistically. As a result, I narrowed the format of my data collection to subreddit comments, since they contained the most raw text that I needed for linguistic analysis.</p>
<p>I assigned variables to the <!-- raw HTML omitted -->PRAW comments()<!-- raw HTML omitted --> function that scraped 300 comments from r/antiwork and r/productivity respectively:</p>
<pre><code># sample 300 comments from each subreddit
antiwork = antiwork.comments(limit=300)
productivity = productivity.comments(limit=300)
</code></pre>
<p>This action doesn’t access the actual text from the comments, only the data objects. So, to get the actual text from the comments, I wrote a general function that can take raw text (comment.body) from each comment for any selected subreddit:</p>
<pre><code># return raw text from comments
def return_comments(community):
  return [comment.body for comment in community if discord_string not in comment.body]
</code></pre>
<p>The result of this was a list of strings that contained the raw text of 300 comments from the selected subreddit.</p>
<p>Then, I applied this function to the 300 sample comments from the two communities:</p>
<pre><code># create corpora training data for each subreddit based on top-level comments
antiwork_corpus = return_comments(antiwork)
productivity_corpus = return_comments(productivity)
</code></pre>
<p>When I printed one of these variables, the result was a list of 300 raw comment strings scraped from the selected subreddit community (the result below is an example from r/antiwork):</p>
<p><img src="/comment_strings.png" alt="Comment strings"></p>
<p>At this point, the raw textual comment data has been pulled from the selected subreddits and put into a list. Each item in this list is a long multi-word string. However, to make sense of data linguistically, we need more than just raw strings of data. We need to tokenize the data, or in other words, split the raw strings up into smaller parts such as words or punctuation. In this way, we can grapple with the linguistically meaningful parts of our data and do some analysis!</p>
<h2 id="step-4-words---tokenizing-and-analysing-parts-of-speech">Step 4: Words - tokenizing and analysing parts-of-speech</h2>
<p>Since I was looking for the top 20 nouns and adjectives in the subreddit communities, it made sense to aim for actual words in my tokenize data (as opposed to sentences or punctuation).</p>
<p>I started off writing a function that used the <!-- raw HTML omitted -->tokenize()<!-- raw HTML omitted --> module from the <!-- raw HTML omitted -->nltk<!-- raw HTML omitted --> library. Since I was also looking at all the words in the corpus (not of any specific user comments), I used the <!-- raw HTML omitted -->“”.join<!-- raw HTML omitted --> Python command to link all the comments of the corpus words together in one big list (as opposed to a list of lists).</p>
<p>I applied this function to the antiwork_corpus and productivity_corpus variables:</p>
<pre><code># tokenize corpora

def tokenize(community):
  return word_tokenize(&quot;&quot;.join(community))

tokenized_antiwork = tokenize(antiwork_corpus)
tokenized_productivity = tokenize(productivity_corpus)
</code></pre>
<p>I applied the <!-- raw HTML omitted -->nltk.pos_tag()<!-- raw HTML omitted --> function to the tokenized data for both subreddits. This would apply a part-of-speech tag (in other words, their linguistic category) to each word in the tokenized list.</p>
<pre><code># tag corpus words for part of speech
tagged_antiwork = pos_tag(tokenized_antiwork)
tagged_productivity = pos_tag(tokenized_productivity)
</code></pre>
<p>The result of this function was a list of tuples where each tuple had the word and their corresponding part-of-speech tag. You can see some examples in the printed list below:</p>
<p><img src="/tagged_tuple_examples.png" alt="Tagged tuble examples"></p>
<p>So, now I was getting somewhere - I had scraped comments, tokenized the data, and assigned part-of-speech tags to the tokenized data.</p>
<h2 id="step-5-frequency-distributions-of-nouns-and-adjectives">Step 5: Frequency distributions of nouns and adjectives</h2>
<p>For this part of the project, I was interested specifically in nouns and adjectives since they would provide more insight into the tone of the subreddits (as opposed to grammatical, functional words like “the” or “an” that don’t provide much meaning or sentiment in themselves).</p>
<p>The <!-- raw HTML omitted -->nltk<!-- raw HTML omitted --> tag set divides up the general categories of nouns and adjectives into many more sub-categories that reflect more detailed elements such as noun singularity or plurality or whether an adjective is comparative or superlative. You can see the various tags in the table below:</p>
<p><img src="/pos_tag_table.png" alt="POS tag table"></p>
<p>For what I was looking for (the top 20 nouns and adjectives in a subreddit), I made a pragmatic decision to lump these sub-categories together into the two general categories of nouns and adjectives. The conversions can be seen in the added third column in the table below.</p>
<p><img src="/pos_recat_table.png" alt="POS retag"></p>
<p>In my code, to re-assign the part-of-speech tags to my more generalised categories, I created lists that lumped the more specific tags together (e.g. ‘NN’ or noun singular or ‘NNPS’ or proper noun plural).</p>
<pre><code># PARTS OF SPEECH

# noun singular, noun plural, proper noun singular, proper noun plural

nouns = ['NN', 'NNS', 'NNP', 'NNPS'] 

# adj, adj comparative, adj superlative

adjs = [&quot;JJ&quot;, &quot;JJR&quot;, &quot;JJS&quot;] 
</code></pre>
<p>In doing this, I would still be using the sub-categories of the <!-- raw HTML omitted -->nltk<!-- raw HTML omitted --> library to pick up nouns and adjectives in the comment data, but I would be putting the words into more general part-of-speech lists.</p>
<p>Next, I had to generate a frequency distribution of the nouns or adjectives in the word data for a given subreddit. The frequency distribution would return information on how many times a particular noun or adjective appeared in the data and list this information alongside the counts of the other nouns or adjectives in the sample</p>
<p>I made a function (<!-- raw HTML omitted -->freq_words<!-- raw HTML omitted -->) that used a list comprehension to pull out word forms according to their corresponding part-of-speech tags.</p>
<pre><code># retrieve freqs for nouns &amp; adjectives

def freq_words(community, pos_tag):
return nltk.FreqDist([x[0] for x in community if x[1] in pos_tag and len(x[0]) &gt; 2 and x[0] != &quot;https&quot;])
</code></pre>
<p>In this function, I made sure that the words pulled would be three or more characters long, to filter out punctuation and grammatical words like ‘the’ or ‘an’. I also filtered out the string “https” since this wasn’t a real word but was often found where a user had posted a link to the community. Finally, I selected the surface form of the word to put into my frequency distribution instead of selecting both the surface form and the <!-- raw HTML omitted -->nltk<!-- raw HTML omitted --> tag (e.g. “JJ”). I thought our data plots would be neater without the sub-category tags.</p>
<p>I applied this function to the noun and adjective word data in r/productivity and r/antiwork:</p>
<pre><code># antiwork frequency distribution (nouns/verbs/adjectives)
antiwork_nouns = freq_words(tagged_antiwork, nouns) 
antiwork_adjs = freq_words(tagged_antiwork, adjs)

# productivity frequency distribution (nouns/verbs/adjectives)productivity_nouns = freq_words(tagged_productivity, nouns)
productivity_adjs = freq_words(tagged_productivity, adjs)
</code></pre>
<p>So, now I had frequency distributions for nouns and adjectives for both the r/antiwork and r/productivity comment data. Now it was time for me to visualise the data and find the most common words in the comments.</p>
<h2 id="step-6-plotting-the-top-20-nouns-and-adjectives-for-each-subreddit">Step 6: Plotting the top 20 nouns and adjectives for each subreddit</h2>
<p>Now that I had frequency distributions for the nouns and adjectives in both r/antiwork and r/productivity, it was time to see what the top 20 words were in these subreddits.</p>
<p>I used the <!-- raw HTML omitted -->pandas<!-- raw HTML omitted --> plot function to plot the top 20 nouns and adjectives for each subreddit:</p>
<pre><code># TOP 20 NOUNS 
productivity_nouns.plot(20, cumulative=False, title=&quot;Top 20 r/productivity nouns&quot;)
antiwork_nouns.plot(20, cumulative=False, title=&quot;Top 20 r/antiwork nouns&quot;)

# TOP 20 ADJECTIVES
productivity_adjs.plot(20, cumulative=False, title=&quot;Top 20 r/productivity adjectives&quot;)
antiwork_adjs.plot(20, cumulative=False, title=&quot;Top 20 r/antiwork adjectives&quot;)
</code></pre>
<p>The result of these were a set of graphs that visually plotted the counts of the top 20 words in the subreddit for each part-of-speech as a decreasing frequency distribution.</p>
<p>Let’s have a look at them below!</p>
<h2 id="insight-1-nouns">Insight 1: Nouns</h2>
<p><img src="/productivity_nouns_plot.png" alt="Productivity nouns plot">
<img src="/antiwork_nouns_plot.png" alt="Antiwork nouns plot"></p>
<p>The top 20 nouns in r/productivity contained topics related to time (‘time’, ‘day’, ‘week’), executive functioning (‘list’, ‘note’/’notes’, ‘habit’, ‘tasks’, ‘calendar’) and objects (‘things’, ‘app’/’apps’, ‘phone’, ‘stuff’). The top 20 nouns in r/antiwork contained more topics related to people (‘people/person’, ‘someone’) and business and societal systems (‘job’/’jobs’, ‘wage’, ‘work’, ‘money’, ‘company’, ‘business’).</p>
<p>Some of the top nouns in this data were related to the concept of time. The top noun lists of r/antiwork and r/productivity both had words related to time. However, the temporal words in r/productivity seemed to relate to an interest in the short-term (&lsquo;day&rsquo;, &lsquo;week&rsquo;) but r/antiwork seemed to relate to an interest in the long-term (&lsquo;years&rsquo;, &lsquo;time&rsquo;, &lsquo;life&rsquo;). You could say that r/productivity and r/antiwork are both interested in time, but r/productivity users focus on the week-by-week perspective while r/antiwork users focus on time on a macro scale.</p>
<p>Another comparison I could make is the use of nouns related to objects as opposed to nouns related to societal systems. The r/productivity data contained more nouns related to objects (e.g. &lsquo;list&rsquo;, &lsquo;phone&rsquo;, &lsquo;stuff&rsquo;, &lsquo;notes&rsquo;), while the r/antiwork data contained more nouns related to people or societal constructs (&lsquo;people&rsquo;, &lsquo;wage&rsquo;, &lsquo;living&rsquo;). This could reflect the tendency of r/productivity users to seek practical tools to complete their work, r/antiwork users to be more interested in critically evaluating the systemic issues that exist in their work in the first place.</p>
<p>Overall, looking at the most common nouns that occur in these two contrasting subreddits can give us some indication of the types of things people tend to talk about in these communities.</p>
<h2 id="insight-2-adjectives">Insight 2: Adjectives</h2>
<p>Looking at adjectives in language data can give us an indication of the emotions of users in a subreddit. Some adjectives connotate positive emotions like joy, excitement, and motivation, while other adjectives can denote negative emotions like anger, resentment, and apathy.</p>
<p><img src="/antiwork_adjectives_plot.png" alt="Antiwork adjs plot">
<img src="/productivity_adjs_plot.png" alt="Productivity adjs plot"></p>
<p>In both subreddits, the top three adjectives are ‘good’, ‘more’ and ‘other’ (with the order of ‘good’ and ‘more’ swapped in r/antiwork). These are highly frequent words in English that can occur in many contexts. This is a problem if we want to discern the connotation associated with the use of these adjectives. For example, you can use the word “good” to denote a positive emotion, such as “this spaghetti is so good” or “you’re a good egg”. However, you can use the word “good” in sentence constructions with neutral or even negative sentiment, such as “it’s a good indication that things will remain uncertain for a while” or “you’re a good liar, aren’t you?”. This issue arises when using single-word data (as opposed to multiple words or sentences). Overall, it’s hard to discern the connotation associated with adjectives without the surrounding context.</p>
<p>Another issue in this frequency analysis is that the sample size of words is smaller for both subreddits – there simply aren’t as many counts per word as what we can see in noun data. For example, the top adjective ‘more’ in r/antiwork has only 19 counts, whereas the top nouns in both subreddits have over 50 counts.</p>
<p>So, I would think of this adjective data as a fun starting point to think about the sentiment in these subreddits, rather than a reliable indication of it.</p>
<h2 id="data-collection--a-caveat">Data collection – a caveat</h2>
<p>This part of my project was a peep into how words are used in the two subreddits using a little bit of scraped comment data. However, there are flaws with what I’ve done here.</p>
<p>One major flaw is that the <!-- raw HTML omitted -->praw<!-- raw HTML omitted --> function I used to scrape the comment bodies simply dives into the subreddit and scrapes any comment body until that comment count reaches 300. Depending on how many submissions were made that day, and how long each thread was, the sampled comment data can end up representing only two or three subreddit threads. I think this is why some of the data I plotted was weirdly specific (e.g., the word ‘Japanese’ appearing in the top 20 adjectives in r/productivity).</p>
<p>Also, when I ran the script on different days (or even hours), the top 20 words changed each time. Perhaps if I changed the script to only include the top 20 nouns or adjectives from “top comments” rather than any comments (and from “top submissions” rather than any submissions), I could get more generalised data that reflected which words are most favoured in usage in these communities.</p>
<h2 id="summary">Summary</h2>
<p>Overall, I had lots of fun finding the top 20 nouns and adjectives for the two subreddits. It was super cool to practise using some powerful Python libraries (<!-- raw HTML omitted -->praw<!-- raw HTML omitted -->, <!-- raw HTML omitted -->nltk<!-- raw HTML omitted -->, <!-- raw HTML omitted -->pandas<!-- raw HTML omitted -->) and to think about the fascinating reasons behind word choice in two very different Reddit communities.</p>
<p>In terms of limitations, I had to keep in mind that my sample size was small and that my script skews my data collection to specific subreddits. I also couldn’t get much out of the adjective data without seeing the surrounding contexts of those adjectives.</p>
<p>This isn’t the only thing I’m doing with Reddit data by the way – I’m doing ongoing research into Reddit language data (including other subreddits) and I’m excited to keep learning and building!</p>
]]></content>
        </item>
        
    </channel>
</rss>
